{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:36.356044Z","iopub.execute_input":"2023-10-02T17:30:36.357148Z","iopub.status.idle":"2023-10-02T17:30:36.383871Z","shell.execute_reply.started":"2023-10-02T17:30:36.357113Z","shell.execute_reply":"2023-10-02T17:30:36.382956Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"code","source":"! cd /kaggle/working/\n! cp -r /kaggle/input/erav1-s17/S17 .","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:36.386371Z","iopub.execute_input":"2023-10-02T17:30:36.386761Z","iopub.status.idle":"2023-10-02T17:30:38.986369Z","shell.execute_reply.started":"2023-10-02T17:30:36.386738Z","shell.execute_reply":"2023-10-02T17:30:38.984756Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/S17","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:38.988392Z","iopub.execute_input":"2023-10-02T17:30:38.989434Z","iopub.status.idle":"2023-10-02T17:30:39.038050Z","shell.execute_reply.started":"2023-10-02T17:30:38.989394Z","shell.execute_reply":"2023-10-02T17:30:39.036974Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/working/S17\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch\n\nfrom src.bert_utils import get_batch, save_model_embeddings\nfrom src.datasets import SentencesDataset, prepare_sentences_dataset\nfrom src.model import Transformer\nfrom src.engines import bert_training_step\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:39.042444Z","iopub.execute_input":"2023-10-02T17:30:39.044933Z","iopub.status.idle":"2023-10-02T17:30:39.083222Z","shell.execute_reply.started":"2023-10-02T17:30:39.044901Z","shell.execute_reply":"2023-10-02T17:30:39.082324Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Config\nbatch_size = 512\nseq_len = 20\nembed_size = 128\ninner_ff_size = embed_size * 4\nn_heads = 8\nn_code = 8\nn_vocab = 40000\ndropout = 0.1\nn_workers = 2\n\n# Optimizer\noptim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:39.086480Z","iopub.execute_input":"2023-10-02T17:30:39.090948Z","iopub.status.idle":"2023-10-02T17:30:39.142104Z","shell.execute_reply.started":"2023-10-02T17:30:39.090916Z","shell.execute_reply":"2023-10-02T17:30:39.136676Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sentences, vocab = prepare_sentences_dataset(dataset_path='data/training.txt', vocab_path='data/vocab.txt', vocab_size=n_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:39.143194Z","iopub.execute_input":"2023-10-02T17:30:39.143749Z","iopub.status.idle":"2023-10-02T17:30:40.855245Z","shell.execute_reply.started":"2023-10-02T17:30:39.143719Z","shell.execute_reply":"2023-10-02T17:30:40.853650Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Loading dataset from data/training.txt\nTokenizing dataset!\nLoading vocabulary from data/vocab.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = SentencesDataset(sentences, vocab, seq_len=seq_len)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:40.856902Z","iopub.execute_input":"2023-10-02T17:30:40.857242Z","iopub.status.idle":"2023-10-02T17:30:40.889495Z","shell.execute_reply.started":"2023-10-02T17:30:40.857211Z","shell.execute_reply":"2023-10-02T17:30:40.888561Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\ndata_loader = DataLoader(dataset, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:40.890685Z","iopub.execute_input":"2023-10-02T17:30:40.891338Z","iopub.status.idle":"2023-10-02T17:30:40.916421Z","shell.execute_reply.started":"2023-10-02T17:30:40.891307Z","shell.execute_reply":"2023-10-02T17:30:40.915524Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = Transformer(\n    embed_dim=embed_size,\n    num_heads=n_heads,\n    attn_dropout=dropout,\n    mlp_dim=inner_ff_size,\n    mlp_dropout=dropout,\n    mlp_activation=nn.ReLU(),\n    num_layers=n_code,\n    embed_dict_size=n_vocab,\n    max_seq_len=seq_len,\n    pad_idx=dataset.IGNORE_IDX,\n    add_cls_token=False,\n    pe_requires_grad=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:40.918013Z","iopub.execute_input":"2023-10-02T17:30:40.918703Z","iopub.status.idle":"2023-10-02T17:30:41.021286Z","shell.execute_reply.started":"2023-10-02T17:30:40.918672Z","shell.execute_reply":"2023-10-02T17:30:41.020328Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.output_linear = nn.Linear(embed_size, n_vocab, bias=False)\n        self.token_embed_layer = model.token_embed_layer\n\n    def forward(self, x, attn_mask):\n        x = self.model(x, attn_mask)\n        return self.output_linear(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:41.022521Z","iopub.execute_input":"2023-10-02T17:30:41.023109Z","iopub.status.idle":"2023-10-02T17:30:41.047796Z","shell.execute_reply.started":"2023-10-02T17:30:41.023079Z","shell.execute_reply":"2023-10-02T17:30:41.046843Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nbert_model = BERT(model)\nbert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:41.049009Z","iopub.execute_input":"2023-10-02T17:30:41.049340Z","iopub.status.idle":"2023-10-02T17:30:41.140095Z","shell.execute_reply.started":"2023-10-02T17:30:41.049310Z","shell.execute_reply":"2023-10-02T17:30:41.139191Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"BERT(\n  (model): Transformer(\n    (mlp_activation): ReLU()\n    (token_embed_layer): Embedding(40000, 128, padding_idx=23945)\n    (pos_embed_layer): PositionalEmbedding()\n    (transformer_blocks): Sequential(\n      (0): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): TransformerBlock(\n        (mha_block): MultiheadSelfAttentionBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (mlp_block): MultiLayerPerceptronBlock(\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Dropout(p=0.1, inplace=False)\n            (3): Linear(in_features=512, out_features=128, bias=True)\n            (4): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (classifier_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (output_linear): Linear(in_features=128, out_features=40000, bias=False)\n  (token_embed_layer): Embedding(40000, 128, padding_idx=23945)\n)"},"metadata":{}}]},{"cell_type":"code","source":"summary(bert_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:41.141238Z","iopub.execute_input":"2023-10-02T17:30:41.141549Z","iopub.status.idle":"2023-10-02T17:30:41.176032Z","shell.execute_reply.started":"2023-10-02T17:30:41.141520Z","shell.execute_reply":"2023-10-02T17:30:41.175045Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"===============================================================================================\nLayer (type:depth-idx)                                                 Param #\n===============================================================================================\nBERT                                                                   --\n├─Transformer: 1-1                                                     --\n│    └─ReLU: 2-1                                                       --\n│    └─Embedding: 2-2                                                  5,120,000\n│    └─PositionalEmbedding: 2-3                                        --\n│    └─Sequential: 2-4                                                 --\n│    │    └─TransformerBlock: 3-1                                      198,272\n│    │    └─TransformerBlock: 3-2                                      198,272\n│    │    └─TransformerBlock: 3-3                                      198,272\n│    │    └─TransformerBlock: 3-4                                      198,272\n│    │    └─TransformerBlock: 3-5                                      198,272\n│    │    └─TransformerBlock: 3-6                                      198,272\n│    │    └─TransformerBlock: 3-7                                      198,272\n│    │    └─TransformerBlock: 3-8                                      198,272\n│    └─LayerNorm: 2-5                                                  256\n├─Linear: 1-2                                                          5,120,000\n├─Embedding: 1-3                                                       (recursive)\n===============================================================================================\nTotal params: 11,826,432\nTrainable params: 11,826,432\nNon-trainable params: 0\n==============================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"optimizer = optim.Adam(bert_model.parameters(), **optim_kwargs)\nloss_fn = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:41.177648Z","iopub.execute_input":"2023-10-02T17:30:41.178420Z","iopub.status.idle":"2023-10-02T17:30:41.202154Z","shell.execute_reply.started":"2023-10-02T17:30:41.178390Z","shell.execute_reply":"2023-10-02T17:30:41.201152Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print('Training...')\nprint_each=10\nmodel.train()\nbatch_iter = iter(data_loader)\nn_iteration = 10000\nfor itr in range(n_iteration):\n    bert_training_step(\n    itr=itr, \n    model=bert_model, \n    data_loader=data_loader, \n    batch_iter=batch_iter, \n    loss_fn=loss_fn, \n    optimizer=optimizer, \n    print_each=print_each,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:30:41.205746Z","iopub.execute_input":"2023-10-02T17:30:41.205979Z","iopub.status.idle":"2023-10-02T17:55:59.884291Z","shell.execute_reply.started":"2023-10-02T17:30:41.205959Z","shell.execute_reply":"2023-10-02T17:55:59.883333Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Training...\nit: 0  | loss 10.72  | Δw: 1.44\nit: 10  | loss 10.05  | Δw: 0.753\nit: 20  | loss 9.79  | Δw: 0.508\nit: 30  | loss 9.67  | Δw: 0.422\nit: 40  | loss 9.52  | Δw: 0.333\nit: 50  | loss 9.33  | Δw: 0.312\nit: 60  | loss 9.1  | Δw: 0.297\nit: 70  | loss 8.97  | Δw: 0.288\nit: 80  | loss 8.76  | Δw: 0.256\nit: 90  | loss 8.63  | Δw: 0.232\nit: 100  | loss 8.49  | Δw: 0.228\nit: 110  | loss 8.36  | Δw: 0.232\nit: 120  | loss 8.15  | Δw: 0.225\nit: 130  | loss 8.05  | Δw: 0.214\nit: 140  | loss 7.95  | Δw: 0.212\nit: 150  | loss 7.77  | Δw: 0.2\nit: 160  | loss 7.67  | Δw: 0.191\nit: 170  | loss 7.55  | Δw: 0.188\nit: 180  | loss 7.41  | Δw: 0.183\nit: 190  | loss 7.29  | Δw: 0.181\nit: 200  | loss 7.22  | Δw: 0.182\nit: 210  | loss 7.19  | Δw: 0.178\nit: 220  | loss 7.03  | Δw: 0.185\nit: 230  | loss 6.92  | Δw: 0.176\nit: 240  | loss 6.88  | Δw: 0.176\nit: 250  | loss 6.85  | Δw: 0.173\nit: 260  | loss 6.9  | Δw: 0.177\nit: 270  | loss 6.79  | Δw: 0.175\nit: 280  | loss 6.84  | Δw: 0.186\nit: 290  | loss 6.77  | Δw: 0.181\nit: 300  | loss 6.62  | Δw: 0.175\nit: 310  | loss 6.57  | Δw: 0.2\nit: 320  | loss 6.48  | Δw: 0.194\nit: 330  | loss 6.57  | Δw: 0.189\nit: 340  | loss 6.44  | Δw: 0.191\nit: 350  | loss 6.56  | Δw: 0.221\nit: 360  | loss 6.56  | Δw: 0.209\nit: 370  | loss 6.48  | Δw: 0.216\nit: 380  | loss 6.52  | Δw: 0.212\nit: 390  | loss 6.46  | Δw: 0.222\nit: 400  | loss 6.4  | Δw: 0.238\nit: 410  | loss 6.37  | Δw: 0.236\nit: 420  | loss 6.47  | Δw: 0.235\nit: 430  | loss 6.4  | Δw: 0.253\nit: 440  | loss 6.41  | Δw: 0.259\nit: 450  | loss 6.39  | Δw: 0.279\nit: 460  | loss 6.38  | Δw: 0.279\nit: 470  | loss 6.35  | Δw: 0.37\nit: 480  | loss 6.43  | Δw: 0.313\nit: 490  | loss 6.34  | Δw: 0.329\nit: 500  | loss 6.32  | Δw: 0.328\nit: 510  | loss 6.32  | Δw: 0.361\nit: 520  | loss 6.3  | Δw: 0.396\nit: 530  | loss 6.38  | Δw: 0.401\nit: 540  | loss 6.36  | Δw: 0.424\nit: 550  | loss 6.45  | Δw: 0.494\nit: 560  | loss 6.34  | Δw: 0.473\nit: 570  | loss 6.54  | Δw: 0.493\nit: 580  | loss 6.44  | Δw: 0.55\nit: 590  | loss 6.32  | Δw: 0.539\nit: 600  | loss 6.24  | Δw: 0.563\nit: 610  | loss 6.33  | Δw: 0.63\nit: 620  | loss 6.41  | Δw: 0.666\nit: 630  | loss 6.33  | Δw: 0.688\nit: 640  | loss 6.23  | Δw: 0.722\nit: 650  | loss 6.33  | Δw: 0.786\nit: 660  | loss 6.27  | Δw: 0.738\nit: 670  | loss 6.37  | Δw: 0.798\nit: 680  | loss 6.36  | Δw: 0.87\nit: 690  | loss 6.23  | Δw: 0.958\nit: 700  | loss 6.26  | Δw: 0.922\nit: 710  | loss 6.34  | Δw: 0.831\nit: 720  | loss 6.41  | Δw: 0.955\nit: 730  | loss 6.23  | Δw: 0.962\nit: 740  | loss 6.34  | Δw: 0.899\nit: 750  | loss 6.26  | Δw: 0.989\nit: 760  | loss 6.32  | Δw: 0.957\nit: 770  | loss 6.32  | Δw: 0.942\nit: 780  | loss 6.33  | Δw: 0.944\nit: 790  | loss 6.29  | Δw: 1.007\nit: 800  | loss 6.37  | Δw: 0.985\nit: 810  | loss 6.33  | Δw: 1.161\nit: 820  | loss 6.22  | Δw: 1.112\nit: 830  | loss 6.42  | Δw: 1.174\nit: 840  | loss 6.2  | Δw: 1.175\nit: 850  | loss 6.27  | Δw: 1.16\nit: 860  | loss 6.35  | Δw: 1.145\nit: 870  | loss 6.27  | Δw: 1.115\nit: 880  | loss 6.35  | Δw: 1.206\nit: 890  | loss 6.32  | Δw: 1.311\nit: 900  | loss 6.33  | Δw: 1.256\nit: 910  | loss 6.25  | Δw: 1.347\nit: 920  | loss 6.33  | Δw: 1.391\nit: 930  | loss 6.26  | Δw: 1.371\nit: 940  | loss 6.22  | Δw: 1.423\nit: 950  | loss 6.2  | Δw: 1.395\nit: 960  | loss 6.17  | Δw: 1.459\nit: 970  | loss 6.3  | Δw: 1.474\nit: 980  | loss 6.16  | Δw: 1.482\nit: 990  | loss 6.25  | Δw: 1.4\nit: 1000  | loss 6.37  | Δw: 1.443\nit: 1010  | loss 6.28  | Δw: 1.494\nit: 1020  | loss 6.33  | Δw: 1.491\nit: 1030  | loss 6.34  | Δw: 1.664\nit: 1040  | loss 6.33  | Δw: 1.575\nit: 1050  | loss 6.2  | Δw: 1.603\nit: 1060  | loss 6.18  | Δw: 1.673\nit: 1070  | loss 6.22  | Δw: 1.586\nit: 1080  | loss 6.15  | Δw: 1.593\nit: 1090  | loss 6.25  | Δw: 1.75\nit: 1100  | loss 6.27  | Δw: 1.644\nit: 1110  | loss 6.25  | Δw: 1.977\nit: 1120  | loss 6.15  | Δw: 1.71\nit: 1130  | loss 6.29  | Δw: 1.775\nit: 1140  | loss 6.15  | Δw: 1.939\nit: 1150  | loss 6.19  | Δw: 1.971\nit: 1160  | loss 6.29  | Δw: 1.975\nit: 1170  | loss 6.13  | Δw: 1.744\nit: 1180  | loss 6.12  | Δw: 1.783\nit: 1190  | loss 6.17  | Δw: 1.822\nit: 1200  | loss 6.11  | Δw: 1.882\nit: 1210  | loss 6.08  | Δw: 1.836\nit: 1220  | loss 6.07  | Δw: 2.101\nit: 1230  | loss 6.26  | Δw: 1.915\nit: 1240  | loss 6.06  | Δw: 2.327\nit: 1250  | loss 6.16  | Δw: 2.131\nit: 1260  | loss 6.07  | Δw: 1.912\nit: 1270  | loss 6.18  | Δw: 2.177\nit: 1280  | loss 6.09  | Δw: 1.914\nit: 1290  | loss 6.14  | Δw: 1.937\nit: 1300  | loss 5.99  | Δw: 2.166\nit: 1310  | loss 6.2  | Δw: 2.0\nit: 1320  | loss 6.0  | Δw: 2.079\nit: 1330  | loss 6.15  | Δw: 2.131\nit: 1340  | loss 6.09  | Δw: 2.037\nit: 1350  | loss 6.24  | Δw: 2.19\nit: 1360  | loss 6.09  | Δw: 2.018\nit: 1370  | loss 6.22  | Δw: 2.281\nit: 1380  | loss 6.17  | Δw: 2.122\nit: 1390  | loss 6.2  | Δw: 2.321\nit: 1400  | loss 6.16  | Δw: 2.167\nit: 1410  | loss 6.11  | Δw: 2.196\nit: 1420  | loss 6.07  | Δw: 2.186\nit: 1430  | loss 6.19  | Δw: 2.189\nit: 1440  | loss 6.16  | Δw: 2.351\nit: 1450  | loss 6.06  | Δw: 2.196\nit: 1460  | loss 6.1  | Δw: 2.212\nit: 1470  | loss 5.98  | Δw: 2.416\nit: 1480  | loss 6.14  | Δw: 2.344\nit: 1490  | loss 5.93  | Δw: 2.476\nit: 1500  | loss 6.01  | Δw: 2.416\nit: 1510  | loss 5.98  | Δw: 2.549\nit: 1520  | loss 5.93  | Δw: 2.727\nit: 1530  | loss 6.14  | Δw: 2.431\nit: 1540  | loss 6.0  | Δw: 2.392\nit: 1550  | loss 6.04  | Δw: 2.517\nit: 1560  | loss 6.2  | Δw: 2.671\nit: 1570  | loss 6.12  | Δw: 2.736\nit: 1580  | loss 6.05  | Δw: 2.67\nit: 1590  | loss 6.04  | Δw: 2.738\nit: 1600  | loss 6.13  | Δw: 2.851\nit: 1610  | loss 6.11  | Δw: 3.269\nit: 1620  | loss 5.99  | Δw: 2.952\nit: 1630  | loss 6.0  | Δw: 2.854\nit: 1640  | loss 6.07  | Δw: 2.926\nit: 1650  | loss 6.05  | Δw: 2.677\nit: 1660  | loss 6.06  | Δw: 2.948\nit: 1670  | loss 5.98  | Δw: 3.079\nit: 1680  | loss 6.05  | Δw: 2.969\nit: 1690  | loss 5.99  | Δw: 2.939\nit: 1700  | loss 5.96  | Δw: 3.114\nit: 1710  | loss 6.02  | Δw: 2.977\nit: 1720  | loss 5.98  | Δw: 2.932\nit: 1730  | loss 5.83  | Δw: 2.931\nit: 1740  | loss 5.9  | Δw: 2.974\nit: 1750  | loss 5.88  | Δw: 3.422\nit: 1760  | loss 5.92  | Δw: 3.098\nit: 1770  | loss 6.02  | Δw: 3.155\nit: 1780  | loss 6.05  | Δw: 3.232\nit: 1790  | loss 5.98  | Δw: 3.257\nit: 1800  | loss 5.97  | Δw: 3.093\nit: 1810  | loss 5.98  | Δw: 3.719\nit: 1820  | loss 5.79  | Δw: 3.328\nit: 1830  | loss 5.98  | Δw: 3.516\nit: 1840  | loss 5.92  | Δw: 3.373\nit: 1850  | loss 5.86  | Δw: 3.814\nit: 1860  | loss 5.94  | Δw: 3.487\nit: 1870  | loss 6.03  | Δw: 3.772\nit: 1880  | loss 5.95  | Δw: 3.32\nit: 1890  | loss 5.79  | Δw: 3.581\nit: 1900  | loss 5.81  | Δw: 3.668\nit: 1910  | loss 5.96  | Δw: 3.894\nit: 1920  | loss 5.77  | Δw: 3.702\nit: 1930  | loss 6.04  | Δw: 3.814\nit: 1940  | loss 5.93  | Δw: 3.959\nit: 1950  | loss 6.0  | Δw: 3.78\nit: 1960  | loss 5.99  | Δw: 3.661\nit: 1970  | loss 5.71  | Δw: 3.809\nit: 1980  | loss 5.74  | Δw: 3.818\nit: 1990  | loss 5.88  | Δw: 3.839\nit: 2000  | loss 5.85  | Δw: 4.158\nit: 2010  | loss 6.01  | Δw: 3.72\nit: 2020  | loss 5.88  | Δw: 3.87\nit: 2030  | loss 5.87  | Δw: 4.194\nit: 2040  | loss 5.85  | Δw: 4.56\nit: 2050  | loss 5.86  | Δw: 4.178\nit: 2060  | loss 5.63  | Δw: 4.2\nit: 2070  | loss 5.79  | Δw: 3.968\nit: 2080  | loss 5.8  | Δw: 4.287\nit: 2090  | loss 5.86  | Δw: 4.029\nit: 2100  | loss 5.67  | Δw: 3.978\nit: 2110  | loss 5.85  | Δw: 4.149\nit: 2120  | loss 5.86  | Δw: 4.197\nit: 2130  | loss 5.87  | Δw: 4.342\nit: 2140  | loss 5.57  | Δw: 4.646\nit: 2150  | loss 5.83  | Δw: 4.206\nit: 2160  | loss 5.75  | Δw: 4.286\nit: 2170  | loss 5.84  | Δw: 4.43\nit: 2180  | loss 5.74  | Δw: 4.643\nit: 2190  | loss 5.71  | Δw: 4.251\nit: 2200  | loss 5.73  | Δw: 4.476\nit: 2210  | loss 5.81  | Δw: 4.799\nit: 2220  | loss 5.75  | Δw: 4.537\nit: 2230  | loss 5.75  | Δw: 4.925\nit: 2240  | loss 5.72  | Δw: 4.586\nit: 2250  | loss 5.81  | Δw: 4.562\nit: 2260  | loss 5.74  | Δw: 4.891\nit: 2270  | loss 5.69  | Δw: 4.526\nit: 2280  | loss 5.73  | Δw: 5.154\nit: 2290  | loss 5.68  | Δw: 5.252\nit: 2300  | loss 5.7  | Δw: 4.935\nit: 2310  | loss 5.6  | Δw: 4.657\nit: 2320  | loss 5.76  | Δw: 5.02\nit: 2330  | loss 5.72  | Δw: 5.029\nit: 2340  | loss 5.93  | Δw: 4.811\nit: 2350  | loss 5.68  | Δw: 4.877\nit: 2360  | loss 5.55  | Δw: 4.712\nit: 2370  | loss 5.65  | Δw: 4.909\nit: 2380  | loss 5.62  | Δw: 5.091\nit: 2390  | loss 5.6  | Δw: 4.696\nit: 2400  | loss 5.71  | Δw: 4.777\nit: 2410  | loss 5.76  | Δw: 4.706\nit: 2420  | loss 5.5  | Δw: 4.803\nit: 2430  | loss 5.7  | Δw: 4.935\nit: 2440  | loss 5.59  | Δw: 4.834\nit: 2450  | loss 5.75  | Δw: 4.808\nit: 2460  | loss 5.55  | Δw: 5.059\nit: 2470  | loss 5.6  | Δw: 5.071\nit: 2480  | loss 5.6  | Δw: 5.132\nit: 2490  | loss 5.57  | Δw: 5.087\nit: 2500  | loss 5.46  | Δw: 5.014\nit: 2510  | loss 5.58  | Δw: 5.441\nit: 2520  | loss 5.48  | Δw: 5.262\nit: 2530  | loss 5.53  | Δw: 5.12\nit: 2540  | loss 5.68  | Δw: 5.196\nit: 2550  | loss 5.59  | Δw: 5.064\nit: 2560  | loss 5.56  | Δw: 5.854\nit: 2570  | loss 5.62  | Δw: 5.573\nit: 2580  | loss 5.42  | Δw: 5.736\nit: 2590  | loss 5.5  | Δw: 5.436\nit: 2600  | loss 5.48  | Δw: 5.986\nit: 2610  | loss 5.58  | Δw: 5.615\nit: 2620  | loss 5.38  | Δw: 5.751\nit: 2630  | loss 5.53  | Δw: 5.589\nit: 2640  | loss 5.54  | Δw: 5.165\nit: 2650  | loss 5.51  | Δw: 5.623\nit: 2660  | loss 5.54  | Δw: 5.756\nit: 2670  | loss 5.55  | Δw: 5.836\nit: 2680  | loss 5.5  | Δw: 5.583\nit: 2690  | loss 5.37  | Δw: 5.431\nit: 2700  | loss 5.48  | Δw: 5.488\nit: 2710  | loss 5.49  | Δw: 5.447\nit: 2720  | loss 5.59  | Δw: 5.763\nit: 2730  | loss 5.59  | Δw: 5.473\nit: 2740  | loss 5.38  | Δw: 6.13\nit: 2750  | loss 5.47  | Δw: 5.875\nit: 2760  | loss 5.48  | Δw: 5.924\nit: 2770  | loss 5.59  | Δw: 5.754\nit: 2780  | loss 5.56  | Δw: 6.471\nit: 2790  | loss 5.43  | Δw: 5.663\nit: 2800  | loss 5.39  | Δw: 5.827\nit: 2810  | loss 5.38  | Δw: 6.045\nit: 2820  | loss 5.5  | Δw: 5.544\nit: 2830  | loss 5.51  | Δw: 5.485\nit: 2840  | loss 5.41  | Δw: 6.333\nit: 2850  | loss 5.41  | Δw: 6.397\nit: 2860  | loss 5.46  | Δw: 5.849\nit: 2870  | loss 5.38  | Δw: 5.868\nit: 2880  | loss 5.4  | Δw: 6.106\nit: 2890  | loss 5.44  | Δw: 6.175\nit: 2900  | loss 5.33  | Δw: 6.092\nit: 2910  | loss 5.34  | Δw: 6.222\nit: 2920  | loss 5.48  | Δw: 6.191\nit: 2930  | loss 5.49  | Δw: 6.112\nit: 2940  | loss 5.48  | Δw: 5.556\nit: 2950  | loss 5.28  | Δw: 6.009\nit: 2960  | loss 5.24  | Δw: 6.102\nit: 2970  | loss 5.34  | Δw: 6.029\nit: 2980  | loss 5.27  | Δw: 6.186\nit: 2990  | loss 5.45  | Δw: 6.083\nit: 3000  | loss 5.4  | Δw: 6.713\nit: 3010  | loss 5.28  | Δw: 6.039\nit: 3020  | loss 5.42  | Δw: 6.385\nit: 3030  | loss 5.23  | Δw: 6.582\nit: 3040  | loss 5.52  | Δw: 6.401\nit: 3050  | loss 5.32  | Δw: 6.132\nit: 3060  | loss 5.29  | Δw: 6.227\nit: 3070  | loss 5.31  | Δw: 6.474\nit: 3080  | loss 5.19  | Δw: 6.34\nit: 3090  | loss 5.3  | Δw: 6.503\nit: 3100  | loss 5.42  | Δw: 6.639\nit: 3110  | loss 5.25  | Δw: 6.422\nit: 3120  | loss 5.17  | Δw: 6.471\nit: 3130  | loss 5.33  | Δw: 6.457\nit: 3140  | loss 5.34  | Δw: 6.66\nit: 3150  | loss 5.42  | Δw: 7.157\nit: 3160  | loss 5.36  | Δw: 6.828\nit: 3170  | loss 5.28  | Δw: 7.082\nit: 3180  | loss 5.26  | Δw: 6.641\nit: 3190  | loss 5.26  | Δw: 6.793\nit: 3200  | loss 5.28  | Δw: 6.287\nit: 3210  | loss 5.22  | Δw: 7.017\nit: 3220  | loss 5.31  | Δw: 6.539\nit: 3230  | loss 5.33  | Δw: 6.8\nit: 3240  | loss 5.21  | Δw: 7.023\nit: 3250  | loss 5.21  | Δw: 6.872\nit: 3260  | loss 5.16  | Δw: 6.851\nit: 3270  | loss 5.17  | Δw: 6.717\nit: 3280  | loss 5.16  | Δw: 6.488\nit: 3290  | loss 5.28  | Δw: 7.01\nit: 3300  | loss 5.36  | Δw: 6.856\nit: 3310  | loss 5.36  | Δw: 6.627\nit: 3320  | loss 5.37  | Δw: 6.92\nit: 3330  | loss 5.3  | Δw: 7.204\nit: 3340  | loss 5.22  | Δw: 6.893\nit: 3350  | loss 5.27  | Δw: 6.782\nit: 3360  | loss 5.31  | Δw: 7.101\nit: 3370  | loss 5.13  | Δw: 7.314\nit: 3380  | loss 5.31  | Δw: 6.626\nit: 3390  | loss 5.13  | Δw: 6.875\nit: 3400  | loss 5.2  | Δw: 6.774\nit: 3410  | loss 5.12  | Δw: 6.91\nit: 3420  | loss 5.16  | Δw: 6.71\nit: 3430  | loss 5.15  | Δw: 6.845\nit: 3440  | loss 5.3  | Δw: 6.903\nit: 3450  | loss 5.04  | Δw: 7.114\nit: 3460  | loss 5.29  | Δw: 6.789\nit: 3470  | loss 5.14  | Δw: 7.083\nit: 3480  | loss 5.2  | Δw: 6.854\nit: 3490  | loss 5.07  | Δw: 6.858\nit: 3500  | loss 5.18  | Δw: 7.082\nit: 3510  | loss 5.21  | Δw: 6.801\nit: 3520  | loss 5.05  | Δw: 7.453\nit: 3530  | loss 4.95  | Δw: 7.104\nit: 3540  | loss 5.1  | Δw: 7.377\nit: 3550  | loss 5.21  | Δw: 7.132\nit: 3560  | loss 5.25  | Δw: 7.205\nit: 3570  | loss 5.24  | Δw: 6.992\nit: 3580  | loss 5.25  | Δw: 7.584\nit: 3590  | loss 5.18  | Δw: 6.878\nit: 3600  | loss 5.26  | Δw: 7.185\nit: 3610  | loss 5.12  | Δw: 7.111\nit: 3620  | loss 5.22  | Δw: 6.964\nit: 3630  | loss 5.13  | Δw: 7.044\nit: 3640  | loss 5.06  | Δw: 6.683\nit: 3650  | loss 5.08  | Δw: 7.276\nit: 3660  | loss 5.17  | Δw: 7.163\nit: 3670  | loss 5.24  | Δw: 7.45\nit: 3680  | loss 5.07  | Δw: 7.456\nit: 3690  | loss 5.1  | Δw: 7.419\nit: 3700  | loss 5.2  | Δw: 7.457\nit: 3710  | loss 5.23  | Δw: 7.421\nit: 3720  | loss 5.25  | Δw: 7.693\nit: 3730  | loss 5.17  | Δw: 7.564\nit: 3740  | loss 5.0  | Δw: 7.63\nit: 3750  | loss 5.29  | Δw: 7.125\nit: 3760  | loss 4.99  | Δw: 7.646\nit: 3770  | loss 5.27  | Δw: 6.998\nit: 3780  | loss 5.13  | Δw: 7.666\nit: 3790  | loss 5.13  | Δw: 7.294\nit: 3800  | loss 5.24  | Δw: 7.211\nit: 3810  | loss 5.17  | Δw: 7.433\nit: 3820  | loss 5.18  | Δw: 7.474\nit: 3830  | loss 5.16  | Δw: 7.115\nit: 3840  | loss 5.12  | Δw: 7.605\nit: 3850  | loss 5.07  | Δw: 7.47\nit: 3860  | loss 5.25  | Δw: 8.019\nit: 3870  | loss 5.18  | Δw: 7.206\nit: 3880  | loss 5.09  | Δw: 7.351\nit: 3890  | loss 4.8  | Δw: 7.47\nit: 3900  | loss 5.1  | Δw: 7.333\nit: 3910  | loss 4.98  | Δw: 7.533\nit: 3920  | loss 4.99  | Δw: 7.216\nit: 3930  | loss 5.16  | Δw: 7.538\nit: 3940  | loss 5.03  | Δw: 7.0\nit: 3950  | loss 4.89  | Δw: 7.394\nit: 3960  | loss 5.12  | Δw: 7.426\nit: 3970  | loss 5.13  | Δw: 7.824\nit: 3980  | loss 5.11  | Δw: 7.265\nit: 3990  | loss 5.18  | Δw: 7.353\nit: 4000  | loss 5.14  | Δw: 7.302\nit: 4010  | loss 4.94  | Δw: 7.789\nit: 4020  | loss 5.1  | Δw: 7.585\nit: 4030  | loss 5.1  | Δw: 7.712\nit: 4040  | loss 5.12  | Δw: 7.29\nit: 4050  | loss 5.12  | Δw: 7.635\nit: 4060  | loss 5.08  | Δw: 7.797\nit: 4070  | loss 5.0  | Δw: 7.684\nit: 4080  | loss 4.9  | Δw: 7.189\nit: 4090  | loss 5.03  | Δw: 7.72\nit: 4100  | loss 5.06  | Δw: 8.007\nit: 4110  | loss 4.79  | Δw: 7.404\nit: 4120  | loss 5.08  | Δw: 7.487\nit: 4130  | loss 4.97  | Δw: 8.012\nit: 4140  | loss 4.98  | Δw: 8.099\nit: 4150  | loss 5.09  | Δw: 7.757\nit: 4160  | loss 4.97  | Δw: 7.944\nit: 4170  | loss 4.93  | Δw: 7.774\nit: 4180  | loss 5.03  | Δw: 7.305\nit: 4190  | loss 5.16  | Δw: 7.778\nit: 4200  | loss 5.05  | Δw: 8.23\nit: 4210  | loss 5.18  | Δw: 7.605\nit: 4220  | loss 5.13  | Δw: 7.801\nit: 4230  | loss 4.96  | Δw: 7.797\nit: 4240  | loss 4.96  | Δw: 7.827\nit: 4250  | loss 4.91  | Δw: 7.982\nit: 4260  | loss 5.17  | Δw: 7.669\nit: 4270  | loss 5.01  | Δw: 7.777\nit: 4280  | loss 4.92  | Δw: 7.775\nit: 4290  | loss 4.97  | Δw: 7.757\nit: 4300  | loss 5.15  | Δw: 7.696\nit: 4310  | loss 5.02  | Δw: 8.38\nit: 4320  | loss 5.18  | Δw: 7.76\nit: 4330  | loss 5.04  | Δw: 7.771\nit: 4340  | loss 5.02  | Δw: 7.818\nit: 4350  | loss 5.15  | Δw: 8.047\nit: 4360  | loss 5.0  | Δw: 8.168\nit: 4370  | loss 4.98  | Δw: 7.72\nit: 4380  | loss 5.09  | Δw: 8.04\nit: 4390  | loss 5.01  | Δw: 7.739\nit: 4400  | loss 4.99  | Δw: 7.848\nit: 4410  | loss 4.98  | Δw: 8.236\nit: 4420  | loss 4.96  | Δw: 7.83\nit: 4430  | loss 4.9  | Δw: 8.022\nit: 4440  | loss 5.04  | Δw: 7.74\nit: 4450  | loss 4.94  | Δw: 7.953\nit: 4460  | loss 5.14  | Δw: 7.944\nit: 4470  | loss 4.91  | Δw: 8.087\nit: 4480  | loss 5.02  | Δw: 8.138\nit: 4490  | loss 5.0  | Δw: 7.833\nit: 4500  | loss 4.88  | Δw: 7.895\nit: 4510  | loss 4.91  | Δw: 7.95\nit: 4520  | loss 4.94  | Δw: 8.55\nit: 4530  | loss 4.88  | Δw: 7.976\nit: 4540  | loss 5.0  | Δw: 8.071\nit: 4550  | loss 4.91  | Δw: 7.983\nit: 4560  | loss 4.93  | Δw: 7.537\nit: 4570  | loss 4.76  | Δw: 8.215\nit: 4580  | loss 4.88  | Δw: 8.608\nit: 4590  | loss 4.87  | Δw: 7.859\nit: 4600  | loss 4.89  | Δw: 8.153\nit: 4610  | loss 4.92  | Δw: 8.214\nit: 4620  | loss 4.93  | Δw: 7.481\nit: 4630  | loss 4.88  | Δw: 8.042\nit: 4640  | loss 5.08  | Δw: 8.048\nit: 4650  | loss 4.91  | Δw: 8.304\nit: 4660  | loss 5.05  | Δw: 8.072\nit: 4670  | loss 4.94  | Δw: 8.229\nit: 4680  | loss 4.81  | Δw: 7.891\nit: 4690  | loss 5.1  | Δw: 8.142\nit: 4700  | loss 4.94  | Δw: 8.33\nit: 4710  | loss 4.74  | Δw: 8.619\nit: 4720  | loss 4.85  | Δw: 8.6\nit: 4730  | loss 4.91  | Δw: 8.464\nit: 4740  | loss 5.02  | Δw: 8.45\nit: 4750  | loss 4.85  | Δw: 8.825\nit: 4760  | loss 4.9  | Δw: 7.869\nit: 4770  | loss 5.12  | Δw: 7.957\nit: 4780  | loss 4.99  | Δw: 8.095\nit: 4790  | loss 4.93  | Δw: 8.635\nit: 4800  | loss 5.03  | Δw: 8.37\nit: 4810  | loss 4.93  | Δw: 8.372\nit: 4820  | loss 4.97  | Δw: 8.691\nit: 4830  | loss 4.9  | Δw: 7.737\nit: 4840  | loss 4.89  | Δw: 8.261\nit: 4850  | loss 4.85  | Δw: 8.834\nit: 4860  | loss 4.88  | Δw: 8.336\nit: 4870  | loss 4.98  | Δw: 8.383\nit: 4880  | loss 5.06  | Δw: 8.371\nit: 4890  | loss 4.9  | Δw: 8.63\nit: 4900  | loss 5.11  | Δw: 8.597\nit: 4910  | loss 4.79  | Δw: 8.92\nit: 4920  | loss 4.9  | Δw: 8.169\nit: 4930  | loss 4.81  | Δw: 8.646\nit: 4940  | loss 4.91  | Δw: 8.403\nit: 4950  | loss 4.91  | Δw: 8.335\nit: 4960  | loss 4.77  | Δw: 7.959\nit: 4970  | loss 4.86  | Δw: 8.621\nit: 4980  | loss 4.92  | Δw: 8.786\nit: 4990  | loss 4.96  | Δw: 8.099\nit: 5000  | loss 4.87  | Δw: 8.508\nit: 5010  | loss 4.89  | Δw: 8.546\nit: 5020  | loss 4.85  | Δw: 9.16\nit: 5030  | loss 4.92  | Δw: 8.39\nit: 5040  | loss 4.89  | Δw: 8.362\nit: 5050  | loss 4.87  | Δw: 8.179\nit: 5060  | loss 4.93  | Δw: 8.108\nit: 5070  | loss 4.92  | Δw: 8.853\nit: 5080  | loss 4.83  | Δw: 8.978\nit: 5090  | loss 4.87  | Δw: 8.113\nit: 5100  | loss 4.84  | Δw: 9.033\nit: 5110  | loss 4.87  | Δw: 8.934\nit: 5120  | loss 4.75  | Δw: 8.781\nit: 5130  | loss 4.8  | Δw: 8.255\nit: 5140  | loss 4.84  | Δw: 8.791\nit: 5150  | loss 4.6  | Δw: 8.555\nit: 5160  | loss 4.86  | Δw: 8.885\nit: 5170  | loss 4.82  | Δw: 8.578\nit: 5180  | loss 4.71  | Δw: 9.248\nit: 5190  | loss 4.74  | Δw: 8.435\nit: 5200  | loss 4.93  | Δw: 8.742\nit: 5210  | loss 4.96  | Δw: 8.795\nit: 5220  | loss 4.87  | Δw: 8.172\nit: 5230  | loss 4.97  | Δw: 8.736\nit: 5240  | loss 4.7  | Δw: 9.161\nit: 5250  | loss 4.84  | Δw: 8.439\nit: 5260  | loss 4.87  | Δw: 8.505\nit: 5270  | loss 4.93  | Δw: 8.266\nit: 5280  | loss 4.79  | Δw: 8.537\nit: 5290  | loss 4.98  | Δw: 9.195\nit: 5300  | loss 4.72  | Δw: 8.77\nit: 5310  | loss 4.7  | Δw: 8.878\nit: 5320  | loss 4.87  | Δw: 8.811\nit: 5330  | loss 4.91  | Δw: 8.757\nit: 5340  | loss 4.66  | Δw: 8.542\nit: 5350  | loss 4.84  | Δw: 8.881\nit: 5360  | loss 4.82  | Δw: 9.041\nit: 5370  | loss 4.84  | Δw: 8.584\nit: 5380  | loss 4.79  | Δw: 8.996\nit: 5390  | loss 4.73  | Δw: 9.048\nit: 5400  | loss 4.73  | Δw: 9.015\nit: 5410  | loss 4.58  | Δw: 9.008\nit: 5420  | loss 4.77  | Δw: 8.712\nit: 5430  | loss 4.77  | Δw: 8.725\nit: 5440  | loss 4.79  | Δw: 8.829\nit: 5450  | loss 4.7  | Δw: 8.967\nit: 5460  | loss 4.78  | Δw: 9.081\nit: 5470  | loss 4.8  | Δw: 8.627\nit: 5480  | loss 4.78  | Δw: 9.305\nit: 5490  | loss 4.82  | Δw: 8.456\nit: 5500  | loss 4.74  | Δw: 8.757\nit: 5510  | loss 4.88  | Δw: 9.113\nit: 5520  | loss 4.81  | Δw: 8.446\nit: 5530  | loss 4.69  | Δw: 8.68\nit: 5540  | loss 4.74  | Δw: 8.832\nit: 5550  | loss 4.72  | Δw: 8.886\nit: 5560  | loss 4.74  | Δw: 8.91\nit: 5570  | loss 4.77  | Δw: 8.823\nit: 5580  | loss 4.84  | Δw: 9.067\nit: 5590  | loss 4.66  | Δw: 8.959\nit: 5600  | loss 4.7  | Δw: 9.16\nit: 5610  | loss 4.8  | Δw: 8.988\nit: 5620  | loss 4.88  | Δw: 8.923\nit: 5630  | loss 4.84  | Δw: 8.823\nit: 5640  | loss 4.71  | Δw: 8.784\nit: 5650  | loss 4.82  | Δw: 8.949\nit: 5660  | loss 4.69  | Δw: 8.584\nit: 5670  | loss 4.96  | Δw: 8.897\nit: 5680  | loss 4.81  | Δw: 8.871\nit: 5690  | loss 4.85  | Δw: 8.858\nit: 5700  | loss 4.73  | Δw: 8.878\nit: 5710  | loss 4.64  | Δw: 8.817\nit: 5720  | loss 4.69  | Δw: 8.641\nit: 5730  | loss 4.78  | Δw: 9.049\nit: 5740  | loss 4.67  | Δw: 8.888\nit: 5750  | loss 4.56  | Δw: 9.553\nit: 5760  | loss 4.8  | Δw: 8.763\nit: 5770  | loss 4.73  | Δw: 9.058\nit: 5780  | loss 4.65  | Δw: 9.28\nit: 5790  | loss 4.6  | Δw: 9.166\nit: 5800  | loss 4.72  | Δw: 8.91\nit: 5810  | loss 4.82  | Δw: 8.667\nit: 5820  | loss 4.75  | Δw: 9.385\nit: 5830  | loss 4.57  | Δw: 8.785\nit: 5840  | loss 4.94  | Δw: 9.006\nit: 5850  | loss 4.58  | Δw: 9.313\nit: 5860  | loss 4.68  | Δw: 9.325\nit: 5870  | loss 4.86  | Δw: 9.501\nit: 5880  | loss 4.66  | Δw: 9.641\nit: 5890  | loss 4.74  | Δw: 9.358\nit: 5900  | loss 4.91  | Δw: 8.727\nit: 5910  | loss 4.79  | Δw: 9.39\nit: 5920  | loss 4.78  | Δw: 8.634\nit: 5930  | loss 4.87  | Δw: 9.417\nit: 5940  | loss 4.9  | Δw: 9.038\nit: 5950  | loss 5.0  | Δw: 9.828\nit: 5960  | loss 4.75  | Δw: 9.325\nit: 5970  | loss 4.61  | Δw: 9.43\nit: 5980  | loss 4.78  | Δw: 9.289\nit: 5990  | loss 4.78  | Δw: 8.668\nit: 6000  | loss 4.84  | Δw: 9.137\nit: 6010  | loss 4.86  | Δw: 9.147\nit: 6020  | loss 4.71  | Δw: 9.492\nit: 6030  | loss 4.67  | Δw: 9.121\nit: 6040  | loss 4.67  | Δw: 9.401\nit: 6050  | loss 4.72  | Δw: 9.579\nit: 6060  | loss 4.79  | Δw: 9.309\nit: 6070  | loss 4.84  | Δw: 8.86\nit: 6080  | loss 4.69  | Δw: 8.451\nit: 6090  | loss 4.66  | Δw: 9.288\nit: 6100  | loss 4.76  | Δw: 9.401\nit: 6110  | loss 4.66  | Δw: 8.946\nit: 6120  | loss 4.54  | Δw: 9.534\nit: 6130  | loss 4.83  | Δw: 9.29\nit: 6140  | loss 4.81  | Δw: 9.216\nit: 6150  | loss 4.65  | Δw: 9.433\nit: 6160  | loss 4.71  | Δw: 9.104\nit: 6170  | loss 4.71  | Δw: 9.654\nit: 6180  | loss 4.75  | Δw: 9.137\nit: 6190  | loss 4.75  | Δw: 9.633\nit: 6200  | loss 4.61  | Δw: 9.425\nit: 6210  | loss 4.69  | Δw: 9.066\nit: 6220  | loss 4.59  | Δw: 9.206\nit: 6230  | loss 4.68  | Δw: 9.417\nit: 6240  | loss 4.7  | Δw: 9.19\nit: 6250  | loss 4.6  | Δw: 9.525\nit: 6260  | loss 4.69  | Δw: 9.095\nit: 6270  | loss 4.69  | Δw: 9.799\nit: 6280  | loss 4.79  | Δw: 9.355\nit: 6290  | loss 4.72  | Δw: 9.063\nit: 6300  | loss 4.54  | Δw: 9.549\nit: 6310  | loss 4.66  | Δw: 10.031\nit: 6320  | loss 4.88  | Δw: 9.676\nit: 6330  | loss 4.74  | Δw: 9.453\nit: 6340  | loss 4.71  | Δw: 9.368\nit: 6350  | loss 4.74  | Δw: 9.845\nit: 6360  | loss 4.74  | Δw: 9.45\nit: 6370  | loss 4.8  | Δw: 9.327\nit: 6380  | loss 4.59  | Δw: 9.436\nit: 6390  | loss 4.8  | Δw: 10.08\nit: 6400  | loss 4.72  | Δw: 9.363\nit: 6410  | loss 4.74  | Δw: 9.48\nit: 6420  | loss 4.7  | Δw: 9.156\nit: 6430  | loss 4.68  | Δw: 9.505\nit: 6440  | loss 4.73  | Δw: 9.124\nit: 6450  | loss 4.69  | Δw: 9.714\nit: 6460  | loss 4.6  | Δw: 9.534\nit: 6470  | loss 4.69  | Δw: 9.47\nit: 6480  | loss 4.7  | Δw: 9.672\nit: 6490  | loss 4.67  | Δw: 9.569\nit: 6500  | loss 4.65  | Δw: 9.75\nit: 6510  | loss 4.7  | Δw: 10.584\nit: 6520  | loss 4.73  | Δw: 9.69\nit: 6530  | loss 4.72  | Δw: 9.854\nit: 6540  | loss 4.73  | Δw: 9.522\nit: 6550  | loss 4.65  | Δw: 9.845\nit: 6560  | loss 4.74  | Δw: 9.777\nit: 6570  | loss 4.88  | Δw: 9.482\nit: 6580  | loss 4.57  | Δw: 9.751\nit: 6590  | loss 4.55  | Δw: 9.866\nit: 6600  | loss 4.8  | Δw: 9.853\nit: 6610  | loss 4.83  | Δw: 9.851\nit: 6620  | loss 4.56  | Δw: 9.609\nit: 6630  | loss 4.72  | Δw: 10.237\nit: 6640  | loss 4.73  | Δw: 9.913\nit: 6650  | loss 4.59  | Δw: 9.443\nit: 6660  | loss 4.95  | Δw: 10.055\nit: 6670  | loss 4.56  | Δw: 9.87\nit: 6680  | loss 4.63  | Δw: 10.032\nit: 6690  | loss 4.72  | Δw: 9.051\nit: 6700  | loss 4.63  | Δw: 9.814\nit: 6710  | loss 4.62  | Δw: 10.165\nit: 6720  | loss 4.71  | Δw: 9.874\nit: 6730  | loss 4.68  | Δw: 9.699\nit: 6740  | loss 4.66  | Δw: 9.719\nit: 6750  | loss 4.44  | Δw: 10.023\nit: 6760  | loss 4.61  | Δw: 10.113\nit: 6770  | loss 4.67  | Δw: 10.135\nit: 6780  | loss 4.72  | Δw: 9.905\nit: 6790  | loss 4.69  | Δw: 9.679\nit: 6800  | loss 4.6  | Δw: 10.203\nit: 6810  | loss 4.75  | Δw: 9.859\nit: 6820  | loss 4.7  | Δw: 9.876\nit: 6830  | loss 4.65  | Δw: 9.994\nit: 6840  | loss 4.64  | Δw: 9.799\nit: 6850  | loss 4.64  | Δw: 10.344\nit: 6860  | loss 4.72  | Δw: 10.031\nit: 6870  | loss 4.48  | Δw: 10.131\nit: 6880  | loss 4.65  | Δw: 10.07\nit: 6890  | loss 4.59  | Δw: 10.002\nit: 6900  | loss 4.78  | Δw: 10.039\nit: 6910  | loss 4.56  | Δw: 10.106\nit: 6920  | loss 4.66  | Δw: 9.823\nit: 6930  | loss 4.83  | Δw: 10.754\nit: 6940  | loss 4.51  | Δw: 9.995\nit: 6950  | loss 4.45  | Δw: 10.15\nit: 6960  | loss 4.54  | Δw: 10.034\nit: 6970  | loss 4.62  | Δw: 9.967\nit: 6980  | loss 4.6  | Δw: 9.585\nit: 6990  | loss 4.69  | Δw: 10.24\nit: 7000  | loss 4.69  | Δw: 9.691\nit: 7010  | loss 4.61  | Δw: 10.077\nit: 7020  | loss 4.69  | Δw: 9.556\nit: 7030  | loss 4.48  | Δw: 9.933\nit: 7040  | loss 4.54  | Δw: 10.01\nit: 7050  | loss 4.48  | Δw: 9.842\nit: 7060  | loss 4.62  | Δw: 10.675\nit: 7070  | loss 4.68  | Δw: 9.956\nit: 7080  | loss 4.65  | Δw: 10.598\nit: 7090  | loss 4.53  | Δw: 10.041\nit: 7100  | loss 4.76  | Δw: 10.438\nit: 7110  | loss 4.67  | Δw: 9.984\nit: 7120  | loss 4.43  | Δw: 10.165\nit: 7130  | loss 4.46  | Δw: 10.092\nit: 7140  | loss 4.6  | Δw: 9.873\nit: 7150  | loss 4.54  | Δw: 10.52\nit: 7160  | loss 4.69  | Δw: 10.401\nit: 7170  | loss 4.69  | Δw: 9.653\nit: 7180  | loss 4.54  | Δw: 10.361\nit: 7190  | loss 4.74  | Δw: 10.234\nit: 7200  | loss 4.77  | Δw: 10.484\nit: 7210  | loss 4.65  | Δw: 9.605\nit: 7220  | loss 4.54  | Δw: 10.534\nit: 7230  | loss 4.53  | Δw: 9.833\nit: 7240  | loss 4.54  | Δw: 10.292\nit: 7250  | loss 4.63  | Δw: 10.567\nit: 7260  | loss 4.77  | Δw: 10.171\nit: 7270  | loss 4.41  | Δw: 9.998\nit: 7280  | loss 4.54  | Δw: 10.563\nit: 7290  | loss 4.4  | Δw: 9.997\nit: 7300  | loss 4.65  | Δw: 10.681\nit: 7310  | loss 4.52  | Δw: 10.158\nit: 7320  | loss 4.52  | Δw: 10.173\nit: 7330  | loss 4.61  | Δw: 10.27\nit: 7340  | loss 4.65  | Δw: 10.462\nit: 7350  | loss 4.5  | Δw: 10.236\nit: 7360  | loss 4.6  | Δw: 10.618\nit: 7370  | loss 4.54  | Δw: 10.825\nit: 7380  | loss 4.66  | Δw: 10.142\nit: 7390  | loss 4.46  | Δw: 10.494\nit: 7400  | loss 4.59  | Δw: 10.651\nit: 7410  | loss 4.39  | Δw: 11.204\nit: 7420  | loss 4.5  | Δw: 11.085\nit: 7430  | loss 4.62  | Δw: 10.698\nit: 7440  | loss 4.49  | Δw: 10.762\nit: 7450  | loss 4.62  | Δw: 10.467\nit: 7460  | loss 4.48  | Δw: 10.63\nit: 7470  | loss 4.53  | Δw: 10.618\nit: 7480  | loss 4.58  | Δw: 10.658\nit: 7490  | loss 4.61  | Δw: 10.585\nit: 7500  | loss 4.69  | Δw: 10.256\nit: 7510  | loss 4.43  | Δw: 10.816\nit: 7520  | loss 4.72  | Δw: 10.65\nit: 7530  | loss 4.39  | Δw: 10.92\nit: 7540  | loss 4.45  | Δw: 10.77\nit: 7550  | loss 4.58  | Δw: 10.724\nit: 7560  | loss 4.47  | Δw: 10.538\nit: 7570  | loss 4.69  | Δw: 11.414\nit: 7580  | loss 4.42  | Δw: 10.326\nit: 7590  | loss 4.6  | Δw: 10.482\nit: 7600  | loss 4.61  | Δw: 10.228\nit: 7610  | loss 4.64  | Δw: 10.785\nit: 7620  | loss 4.4  | Δw: 10.533\nit: 7630  | loss 4.53  | Δw: 10.535\nit: 7640  | loss 4.42  | Δw: 10.598\nit: 7650  | loss 4.54  | Δw: 10.491\nit: 7660  | loss 4.67  | Δw: 10.392\nit: 7670  | loss 4.73  | Δw: 10.305\nit: 7680  | loss 4.52  | Δw: 10.808\nit: 7690  | loss 4.55  | Δw: 10.656\nit: 7700  | loss 4.51  | Δw: 11.412\nit: 7710  | loss 4.41  | Δw: 10.61\nit: 7720  | loss 4.64  | Δw: 10.871\nit: 7730  | loss 4.42  | Δw: 10.47\nit: 7740  | loss 4.47  | Δw: 11.15\nit: 7750  | loss 4.47  | Δw: 10.734\nit: 7760  | loss 4.53  | Δw: 10.751\nit: 7770  | loss 4.67  | Δw: 10.896\nit: 7780  | loss 4.7  | Δw: 11.409\nit: 7790  | loss 4.51  | Δw: 11.16\nit: 7800  | loss 4.49  | Δw: 10.993\nit: 7810  | loss 4.58  | Δw: 10.594\nit: 7820  | loss 4.73  | Δw: 10.532\nit: 7830  | loss 4.56  | Δw: 10.495\nit: 7840  | loss 4.49  | Δw: 10.603\nit: 7850  | loss 4.63  | Δw: 10.782\nit: 7860  | loss 4.41  | Δw: 10.57\nit: 7870  | loss 4.6  | Δw: 11.168\nit: 7880  | loss 4.46  | Δw: 10.842\nit: 7890  | loss 4.6  | Δw: 10.806\nit: 7900  | loss 4.46  | Δw: 11.066\nit: 7910  | loss 4.63  | Δw: 10.617\nit: 7920  | loss 4.43  | Δw: 10.818\nit: 7930  | loss 4.53  | Δw: 11.048\nit: 7940  | loss 4.73  | Δw: 10.78\nit: 7950  | loss 4.37  | Δw: 11.31\nit: 7960  | loss 4.47  | Δw: 10.374\nit: 7970  | loss 4.58  | Δw: 11.186\nit: 7980  | loss 4.59  | Δw: 10.559\nit: 7990  | loss 4.53  | Δw: 11.101\nit: 8000  | loss 4.48  | Δw: 10.947\nit: 8010  | loss 4.57  | Δw: 10.878\nit: 8020  | loss 4.55  | Δw: 11.166\nit: 8030  | loss 4.52  | Δw: 10.622\nit: 8040  | loss 4.45  | Δw: 11.314\nit: 8050  | loss 4.43  | Δw: 10.731\nit: 8060  | loss 4.49  | Δw: 10.852\nit: 8070  | loss 4.54  | Δw: 10.68\nit: 8080  | loss 4.44  | Δw: 10.924\nit: 8090  | loss 4.64  | Δw: 10.937\nit: 8100  | loss 4.48  | Δw: 10.695\nit: 8110  | loss 4.56  | Δw: 11.658\nit: 8120  | loss 4.56  | Δw: 11.084\nit: 8130  | loss 4.49  | Δw: 11.029\nit: 8140  | loss 4.4  | Δw: 11.512\nit: 8150  | loss 4.5  | Δw: 10.834\nit: 8160  | loss 4.49  | Δw: 10.518\nit: 8170  | loss 4.49  | Δw: 11.492\nit: 8180  | loss 4.56  | Δw: 10.842\nit: 8190  | loss 4.49  | Δw: 11.316\nit: 8200  | loss 4.46  | Δw: 11.044\nit: 8210  | loss 4.35  | Δw: 10.838\nit: 8220  | loss 4.62  | Δw: 11.584\nit: 8230  | loss 4.55  | Δw: 11.047\nit: 8240  | loss 4.59  | Δw: 11.909\nit: 8250  | loss 4.46  | Δw: 11.287\nit: 8260  | loss 4.56  | Δw: 11.464\nit: 8270  | loss 4.54  | Δw: 11.419\nit: 8280  | loss 4.54  | Δw: 11.114\nit: 8290  | loss 4.59  | Δw: 11.334\nit: 8300  | loss 4.46  | Δw: 10.819\nit: 8310  | loss 4.55  | Δw: 10.858\nit: 8320  | loss 4.66  | Δw: 11.667\nit: 8330  | loss 4.52  | Δw: 11.635\nit: 8340  | loss 4.43  | Δw: 10.779\nit: 8350  | loss 4.4  | Δw: 11.197\nit: 8360  | loss 4.45  | Δw: 11.565\nit: 8370  | loss 4.57  | Δw: 11.049\nit: 8380  | loss 4.56  | Δw: 11.239\nit: 8390  | loss 4.52  | Δw: 11.147\nit: 8400  | loss 4.47  | Δw: 11.01\nit: 8410  | loss 4.48  | Δw: 11.743\nit: 8420  | loss 4.42  | Δw: 10.683\nit: 8430  | loss 4.45  | Δw: 11.164\nit: 8440  | loss 4.38  | Δw: 11.444\nit: 8450  | loss 4.39  | Δw: 11.284\nit: 8460  | loss 4.43  | Δw: 11.806\nit: 8470  | loss 4.5  | Δw: 11.063\nit: 8480  | loss 4.47  | Δw: 12.056\nit: 8490  | loss 4.43  | Δw: 11.747\nit: 8500  | loss 4.45  | Δw: 11.36\nit: 8510  | loss 4.38  | Δw: 11.251\nit: 8520  | loss 4.45  | Δw: 11.468\nit: 8530  | loss 4.5  | Δw: 11.041\nit: 8540  | loss 4.51  | Δw: 11.393\nit: 8550  | loss 4.63  | Δw: 11.957\nit: 8560  | loss 4.55  | Δw: 11.063\nit: 8570  | loss 4.31  | Δw: 12.274\nit: 8580  | loss 4.55  | Δw: 11.137\nit: 8590  | loss 4.36  | Δw: 11.853\nit: 8600  | loss 4.6  | Δw: 11.898\nit: 8610  | loss 4.53  | Δw: 11.149\nit: 8620  | loss 4.36  | Δw: 11.709\nit: 8630  | loss 4.56  | Δw: 11.584\nit: 8640  | loss 4.45  | Δw: 11.125\nit: 8650  | loss 4.52  | Δw: 11.744\nit: 8660  | loss 4.39  | Δw: 11.282\nit: 8670  | loss 4.34  | Δw: 11.409\nit: 8680  | loss 4.43  | Δw: 11.766\nit: 8690  | loss 4.58  | Δw: 11.417\nit: 8700  | loss 4.47  | Δw: 11.73\nit: 8710  | loss 4.31  | Δw: 11.499\nit: 8720  | loss 4.56  | Δw: 11.733\nit: 8730  | loss 4.46  | Δw: 11.989\nit: 8740  | loss 4.32  | Δw: 11.787\nit: 8750  | loss 4.46  | Δw: 12.053\nit: 8760  | loss 4.71  | Δw: 12.206\nit: 8770  | loss 4.6  | Δw: 10.941\nit: 8780  | loss 4.27  | Δw: 11.798\nit: 8790  | loss 4.55  | Δw: 11.412\nit: 8800  | loss 4.43  | Δw: 11.725\nit: 8810  | loss 4.36  | Δw: 11.797\nit: 8820  | loss 4.38  | Δw: 11.585\nit: 8830  | loss 4.41  | Δw: 12.003\nit: 8840  | loss 4.5  | Δw: 11.842\nit: 8850  | loss 4.24  | Δw: 11.474\nit: 8860  | loss 4.43  | Δw: 11.875\nit: 8870  | loss 4.45  | Δw: 11.331\nit: 8880  | loss 4.47  | Δw: 11.844\nit: 8890  | loss 4.45  | Δw: 11.656\nit: 8900  | loss 4.47  | Δw: 11.709\nit: 8910  | loss 4.67  | Δw: 11.484\nit: 8920  | loss 4.5  | Δw: 12.0\nit: 8930  | loss 4.4  | Δw: 12.12\nit: 8940  | loss 4.51  | Δw: 11.45\nit: 8950  | loss 4.46  | Δw: 12.442\nit: 8960  | loss 4.33  | Δw: 11.855\nit: 8970  | loss 4.48  | Δw: 12.239\nit: 8980  | loss 4.54  | Δw: 11.824\nit: 8990  | loss 4.31  | Δw: 11.508\nit: 9000  | loss 4.39  | Δw: 11.509\nit: 9010  | loss 4.47  | Δw: 12.076\nit: 9020  | loss 4.43  | Δw: 12.514\nit: 9030  | loss 4.35  | Δw: 11.198\nit: 9040  | loss 4.44  | Δw: 12.531\nit: 9050  | loss 4.48  | Δw: 11.5\nit: 9060  | loss 4.42  | Δw: 11.833\nit: 9070  | loss 4.46  | Δw: 12.736\nit: 9080  | loss 4.44  | Δw: 12.165\nit: 9090  | loss 4.44  | Δw: 12.193\nit: 9100  | loss 4.48  | Δw: 11.598\nit: 9110  | loss 4.37  | Δw: 12.063\nit: 9120  | loss 4.51  | Δw: 11.885\nit: 9130  | loss 4.45  | Δw: 12.353\nit: 9140  | loss 4.43  | Δw: 11.85\nit: 9150  | loss 4.39  | Δw: 12.269\nit: 9160  | loss 4.39  | Δw: 11.942\nit: 9170  | loss 4.36  | Δw: 12.306\nit: 9180  | loss 4.4  | Δw: 12.183\nit: 9190  | loss 4.48  | Δw: 12.15\nit: 9200  | loss 4.42  | Δw: 12.272\nit: 9210  | loss 4.42  | Δw: 12.253\nit: 9220  | loss 4.46  | Δw: 13.418\nit: 9230  | loss 4.4  | Δw: 12.831\nit: 9240  | loss 4.39  | Δw: 12.136\nit: 9250  | loss 4.38  | Δw: 13.013\nit: 9260  | loss 4.37  | Δw: 12.107\nit: 9270  | loss 4.44  | Δw: 12.382\nit: 9280  | loss 4.32  | Δw: 12.364\nit: 9290  | loss 4.47  | Δw: 11.497\nit: 9300  | loss 4.34  | Δw: 11.932\nit: 9310  | loss 4.39  | Δw: 12.593\nit: 9320  | loss 4.46  | Δw: 12.455\nit: 9330  | loss 4.39  | Δw: 12.459\nit: 9340  | loss 4.33  | Δw: 12.311\nit: 9350  | loss 4.35  | Δw: 12.406\nit: 9360  | loss 4.51  | Δw: 12.365\nit: 9370  | loss 4.51  | Δw: 12.074\nit: 9380  | loss 4.47  | Δw: 11.953\nit: 9390  | loss 4.46  | Δw: 12.269\nit: 9400  | loss 4.17  | Δw: 12.16\nit: 9410  | loss 4.38  | Δw: 11.511\nit: 9420  | loss 4.18  | Δw: 12.128\nit: 9430  | loss 4.36  | Δw: 11.96\nit: 9440  | loss 4.4  | Δw: 12.873\nit: 9450  | loss 4.38  | Δw: 12.159\nit: 9460  | loss 4.58  | Δw: 13.319\nit: 9470  | loss 4.43  | Δw: 12.513\nit: 9480  | loss 4.55  | Δw: 11.999\nit: 9490  | loss 4.28  | Δw: 12.732\nit: 9500  | loss 4.34  | Δw: 12.097\nit: 9510  | loss 4.46  | Δw: 12.751\nit: 9520  | loss 4.49  | Δw: 12.123\nit: 9530  | loss 4.32  | Δw: 12.09\nit: 9540  | loss 4.5  | Δw: 12.6\nit: 9550  | loss 4.38  | Δw: 12.654\nit: 9560  | loss 4.53  | Δw: 12.361\nit: 9570  | loss 4.37  | Δw: 12.642\nit: 9580  | loss 4.41  | Δw: 12.462\nit: 9590  | loss 4.53  | Δw: 12.512\nit: 9600  | loss 4.46  | Δw: 13.208\nit: 9610  | loss 4.28  | Δw: 13.396\nit: 9620  | loss 4.36  | Δw: 12.489\nit: 9630  | loss 4.28  | Δw: 12.354\nit: 9640  | loss 4.38  | Δw: 12.494\nit: 9650  | loss 4.45  | Δw: 12.74\nit: 9660  | loss 4.33  | Δw: 13.338\nit: 9670  | loss 4.31  | Δw: 11.995\nit: 9680  | loss 4.33  | Δw: 12.443\nit: 9690  | loss 4.48  | Δw: 12.529\nit: 9700  | loss 4.27  | Δw: 13.019\nit: 9710  | loss 4.51  | Δw: 12.768\nit: 9720  | loss 4.45  | Δw: 12.266\nit: 9730  | loss 4.33  | Δw: 12.358\nit: 9740  | loss 4.45  | Δw: 13.142\nit: 9750  | loss 4.33  | Δw: 12.587\nit: 9760  | loss 4.49  | Δw: 12.97\nit: 9770  | loss 4.4  | Δw: 12.928\nit: 9780  | loss 4.44  | Δw: 12.851\nit: 9790  | loss 4.24  | Δw: 12.528\nit: 9800  | loss 4.37  | Δw: 12.896\nit: 9810  | loss 4.3  | Δw: 12.968\nit: 9820  | loss 4.41  | Δw: 12.912\nit: 9830  | loss 4.38  | Δw: 12.933\nit: 9840  | loss 4.38  | Δw: 12.847\nit: 9850  | loss 4.19  | Δw: 12.809\nit: 9860  | loss 4.36  | Δw: 12.819\nit: 9870  | loss 4.3  | Δw: 12.916\nit: 9880  | loss 4.3  | Δw: 13.15\nit: 9890  | loss 4.52  | Δw: 12.605\nit: 9900  | loss 4.37  | Δw: 12.817\nit: 9910  | loss 4.45  | Δw: 12.639\nit: 9920  | loss 4.41  | Δw: 13.198\nit: 9930  | loss 4.29  | Δw: 13.123\nit: 9940  | loss 4.39  | Δw: 13.814\nit: 9950  | loss 4.39  | Δw: 12.96\nit: 9960  | loss 4.3  | Δw: 12.604\nit: 9970  | loss 4.33  | Δw: 13.448\nit: 9980  | loss 4.44  | Δw: 12.589\nit: 9990  | loss 4.3  | Δw: 13.889\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}