{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:36.785619Z","iopub.execute_input":"2023-09-24T15:08:36.786049Z","iopub.status.idle":"2023-09-24T15:08:36.825838Z","shell.execute_reply.started":"2023-09-24T15:08:36.786017Z","shell.execute_reply":"2023-09-24T15:08:36.824593Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"code","source":"! cd /kaggle/working/\n! cp -r /kaggle/input/erav1-s17/S17 .","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:36.827961Z","iopub.execute_input":"2023-09-24T15:08:36.828514Z","iopub.status.idle":"2023-09-24T15:08:39.429470Z","shell.execute_reply.started":"2023-09-24T15:08:36.828480Z","shell.execute_reply":"2023-09-24T15:08:39.428193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/S17","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.431448Z","iopub.execute_input":"2023-09-24T15:08:39.432191Z","iopub.status.idle":"2023-09-24T15:08:39.473222Z","shell.execute_reply.started":"2023-09-24T15:08:39.432149Z","shell.execute_reply":"2023-09-24T15:08:39.472284Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/working/S17\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nfrom torch import nn\nfrom torchvision import transforms\nimport torch.nn.functional as F\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\nfrom src.model import Transformer","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.479080Z","iopub.execute_input":"2023-09-24T15:08:39.479801Z","iopub.status.idle":"2023-09-24T15:08:39.520674Z","shell.execute_reply.started":"2023-09-24T15:08:39.479767Z","shell.execute_reply":"2023-09-24T15:08:39.519689Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"image_path = \"data/pizza_steak_sushi\"\ntrain_dir = image_path + \"/train\"\ntest_dir = image_path + \"/test\"","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.523896Z","iopub.execute_input":"2023-09-24T15:08:39.524319Z","iopub.status.idle":"2023-09-24T15:08:39.558173Z","shell.execute_reply.started":"2023-09-24T15:08:39.524288Z","shell.execute_reply":"2023-09-24T15:08:39.557198Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Create image size (from Table 3 in the ViT paper) \nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.559753Z","iopub.execute_input":"2023-09-24T15:08:39.560066Z","iopub.status.idle":"2023-09-24T15:08:39.595850Z","shell.execute_reply.started":"2023-09-24T15:08:39.560036Z","shell.execute_reply":"2023-09-24T15:08:39.594873Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n    ToTensor()\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.597874Z","iopub.execute_input":"2023-09-24T15:08:39.598593Z","iopub.status.idle":"2023-09-24T15:08:39.634829Z","shell.execute_reply.started":"2023-09-24T15:08:39.598560Z","shell.execute_reply":"2023-09-24T15:08:39.633853Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.638160Z","iopub.execute_input":"2023-09-24T15:08:39.638453Z","iopub.status.idle":"2023-09-24T15:08:39.677373Z","shell.execute_reply.started":"2023-09-24T15:08:39.638430Z","shell.execute_reply":"2023-09-24T15:08:39.676470Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x78738c403a00>,\n <torch.utils.data.dataloader.DataLoader at 0x7872d8c84e50>,\n ['pizza', 'steak', 'sushi'])"},"metadata":{}}]},{"cell_type":"code","source":"# 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n    \n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\" \n    # 2. Initialize the class with appropriate variables\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n        self.patch_size = patch_size\n        \n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method \n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n        \n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n        # 6. Make sure the output shape has the right order \n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.680719Z","iopub.execute_input":"2023-09-24T15:08:39.680983Z","iopub.status.idle":"2023-09-24T15:08:39.719403Z","shell.execute_reply.started":"2023-09-24T15:08:39.680960Z","shell.execute_reply":"2023-09-24T15:08:39.718400Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\nsummary(PatchEmbedding(), \n        input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.724839Z","iopub.execute_input":"2023-09-24T15:08:39.725100Z","iopub.status.idle":"2023-09-24T15:08:39.772921Z","shell.execute_reply.started":"2023-09-24T15:08:39.725078Z","shell.execute_reply":"2023-09-24T15:08:39.772011Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nPatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True\n├─Conv2d (patcher)                       [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n├─Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --\n========================================================================================================================\nTotal params: 590,592\nTrainable params: 590,592\nNon-trainable params: 0\nTotal mult-adds (M): 115.76\n========================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 1.20\nParams size (MB): 2.36\nEstimated Total Size (MB): 4.17\n========================================================================================================================"},"metadata":{}}]},{"cell_type":"code","source":"model = Transformer(\n    embed_dim=768,\n    num_heads=12,\n    attn_dropout=0,\n    mlp_dim=3072,\n    mlp_dropout=0.1,\n    mlp_activation=nn.GELU(),\n    num_layers=12,\n    embed_dict_size=None,\n    max_seq_len=((224 // 16)**2 + 1),\n    pad_idx=None,\n    add_cls_token=True,\n    pe_requires_grad=True,\n    need_embedding=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:39.774182Z","iopub.execute_input":"2023-09-24T15:08:39.774535Z","iopub.status.idle":"2023-09-24T15:08:41.619520Z","shell.execute_reply.started":"2023-09-24T15:08:39.774504Z","shell.execute_reply":"2023-09-24T15:08:41.618479Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.620940Z","iopub.execute_input":"2023-09-24T15:08:41.621394Z","iopub.status.idle":"2023-09-24T15:08:41.659720Z","shell.execute_reply.started":"2023-09-24T15:08:41.621358Z","shell.execute_reply":"2023-09-24T15:08:41.658726Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (mlp_activation): GELU(approximate='none')\n  (pos_embed_layer): PositionalEmbedding()\n  (cls_embed_layer): Embedding(1, 768)\n  (transformer_blocks): Sequential(\n    (0): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (8): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (9): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (10): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (11): TransformerBlock(\n      (mha_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (mlp_block): MultiLayerPerceptronBlock(\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (classifier_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"summary(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.661213Z","iopub.execute_input":"2023-09-24T15:08:41.661572Z","iopub.status.idle":"2023-09-24T15:08:41.715156Z","shell.execute_reply.started":"2023-09-24T15:08:41.661539Z","shell.execute_reply":"2023-09-24T15:08:41.714038Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                                            Param #\n==========================================================================================\nTransformer                                                       --\n├─GELU: 1-1                                                       --\n├─PositionalEmbedding: 1-2                                        --\n├─Embedding: 1-3                                                  768\n├─Sequential: 1-4                                                 --\n│    └─TransformerBlock: 2-1                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-1                      2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-2                        4,723,968\n│    └─TransformerBlock: 2-2                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-3                      2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-4                        4,723,968\n│    └─TransformerBlock: 2-3                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-5                      2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-6                        4,723,968\n│    └─TransformerBlock: 2-4                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-7                      2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-8                        4,723,968\n│    └─TransformerBlock: 2-5                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-9                      2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-10                       4,723,968\n│    └─TransformerBlock: 2-6                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-11                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-12                       4,723,968\n│    └─TransformerBlock: 2-7                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-13                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-14                       4,723,968\n│    └─TransformerBlock: 2-8                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-15                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-16                       4,723,968\n│    └─TransformerBlock: 2-9                                      --\n│    │    └─MultiheadSelfAttentionBlock: 3-17                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-18                       4,723,968\n│    └─TransformerBlock: 2-10                                     --\n│    │    └─MultiheadSelfAttentionBlock: 3-19                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-20                       4,723,968\n│    └─TransformerBlock: 2-11                                     --\n│    │    └─MultiheadSelfAttentionBlock: 3-21                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-22                       4,723,968\n│    └─TransformerBlock: 2-12                                     --\n│    │    └─MultiheadSelfAttentionBlock: 3-23                     2,363,904\n│    │    └─MultiLayerPerceptronBlock: 3-24                       4,723,968\n├─LayerNorm: 1-5                                                  1,536\n==========================================================================================\nTotal params: 85,056,768\nTrainable params: 85,056,768\nNon-trainable params: 0\n=========================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"# Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\nsummary(PatchEmbedding(), \n        input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.716751Z","iopub.execute_input":"2023-09-24T15:08:41.717092Z","iopub.status.idle":"2023-09-24T15:08:41.763558Z","shell.execute_reply.started":"2023-09-24T15:08:41.717060Z","shell.execute_reply":"2023-09-24T15:08:41.762520Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nPatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True\n├─Conv2d (patcher)                       [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n├─Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --\n========================================================================================================================\nTotal params: 590,592\nTrainable params: 590,592\nNon-trainable params: 0\nTotal mult-adds (M): 115.76\n========================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 1.20\nParams size (MB): 2.36\nEstimated Total Size (MB): 4.17\n========================================================================================================================"},"metadata":{}}]},{"cell_type":"code","source":"class VIT(nn.Module):\n    def __init__(self, model, embed_dim, num_classes):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(in_channels=3, patch_size=16, embedding_dim=embed_dim)\n        self.model = model\n        self.linear = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embedding(x)\n\n        x = self.model(x, attn_mask=None)\n        x = self.linear(x[:, 0])\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.765021Z","iopub.execute_input":"2023-09-24T15:08:41.765583Z","iopub.status.idle":"2023-09-24T15:08:41.801044Z","shell.execute_reply.started":"2023-09-24T15:08:41.765551Z","shell.execute_reply":"2023-09-24T15:08:41.800086Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"vit = VIT(model, embed_dim=768, num_classes=len(class_names))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.802382Z","iopub.execute_input":"2023-09-24T15:08:41.802862Z","iopub.status.idle":"2023-09-24T15:08:41.841541Z","shell.execute_reply.started":"2023-09-24T15:08:41.802830Z","shell.execute_reply":"2023-09-24T15:08:41.840493Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"summary(vit)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.843304Z","iopub.execute_input":"2023-09-24T15:08:41.843743Z","iopub.status.idle":"2023-09-24T15:08:41.902098Z","shell.execute_reply.started":"2023-09-24T15:08:41.843712Z","shell.execute_reply":"2023-09-24T15:08:41.901174Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"===============================================================================================\nLayer (type:depth-idx)                                                 Param #\n===============================================================================================\nVIT                                                                    --\n├─PatchEmbedding: 1-1                                                  --\n│    └─Conv2d: 2-1                                                     590,592\n│    └─Flatten: 2-2                                                    --\n├─Transformer: 1-2                                                     --\n│    └─GELU: 2-3                                                       --\n│    └─PositionalEmbedding: 2-4                                        --\n│    └─Embedding: 2-5                                                  768\n│    └─Sequential: 2-6                                                 --\n│    │    └─TransformerBlock: 3-1                                      7,087,872\n│    │    └─TransformerBlock: 3-2                                      7,087,872\n│    │    └─TransformerBlock: 3-3                                      7,087,872\n│    │    └─TransformerBlock: 3-4                                      7,087,872\n│    │    └─TransformerBlock: 3-5                                      7,087,872\n│    │    └─TransformerBlock: 3-6                                      7,087,872\n│    │    └─TransformerBlock: 3-7                                      7,087,872\n│    │    └─TransformerBlock: 3-8                                      7,087,872\n│    │    └─TransformerBlock: 3-9                                      7,087,872\n│    │    └─TransformerBlock: 3-10                                     7,087,872\n│    │    └─TransformerBlock: 3-11                                     7,087,872\n│    │    └─TransformerBlock: 3-12                                     7,087,872\n│    └─LayerNorm: 2-7                                                  1,536\n├─Linear: 1-3                                                          2,307\n===============================================================================================\nTotal params: 85,649,667\nTrainable params: 85,649,667\nNon-trainable params: 0\n==============================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"from super_repo import data_setup, engine, utils\n# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \noptimizer = torch.optim.Adam(params=vit.parameters(), \n                             lr=3e-4, # Base LR from Table 3 for ViT-* ImageNet-1k\n                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n\n# Setup the loss function for multi-class classification\nloss_fn = torch.nn.CrossEntropyLoss()\ndevice=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Train the model and save the training results to a dictionary\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=100,\n                       device=device)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:08:41.903514Z","iopub.execute_input":"2023-09-24T15:08:41.903956Z","iopub.status.idle":"2023-09-24T15:12:49.193520Z","shell.execute_reply.started":"2023-09-24T15:08:41.903923Z","shell.execute_reply":"2023-09-24T15:12:49.192100Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2ac6aedcb7410b991c7467eba6e271"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 3.4481 | train_acc: 0.2539 | test_loss: 1.2632 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.5748 | train_acc: 0.2852 | test_loss: 1.0242 | test_acc: 0.5417\nEpoch: 3 | train_loss: 1.5433 | train_acc: 0.2812 | test_loss: 1.0469 | test_acc: 0.5417\nEpoch: 4 | train_loss: 1.5354 | train_acc: 0.2656 | test_loss: 1.1309 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.2837 | train_acc: 0.2930 | test_loss: 1.5298 | test_acc: 0.1979\nEpoch: 6 | train_loss: 1.2606 | train_acc: 0.4570 | test_loss: 1.1664 | test_acc: 0.5417\nEpoch: 7 | train_loss: 1.2223 | train_acc: 0.4453 | test_loss: 1.2570 | test_acc: 0.2604\nEpoch: 8 | train_loss: 1.3074 | train_acc: 0.2773 | test_loss: 1.3663 | test_acc: 0.1979\nEpoch: 9 | train_loss: 1.1170 | train_acc: 0.3828 | test_loss: 1.0790 | test_acc: 0.5417\nEpoch: 10 | train_loss: 1.2714 | train_acc: 0.4141 | test_loss: 1.8607 | test_acc: 0.2604\nEpoch: 11 | train_loss: 1.4248 | train_acc: 0.2578 | test_loss: 1.3159 | test_acc: 0.1979\nEpoch: 12 | train_loss: 1.2610 | train_acc: 0.3203 | test_loss: 1.0612 | test_acc: 0.5417\nEpoch: 13 | train_loss: 1.0860 | train_acc: 0.4180 | test_loss: 1.2093 | test_acc: 0.2604\nEpoch: 14 | train_loss: 1.1503 | train_acc: 0.2734 | test_loss: 1.0558 | test_acc: 0.5417\nEpoch: 15 | train_loss: 1.1964 | train_acc: 0.2930 | test_loss: 1.1537 | test_acc: 0.2604\nEpoch: 16 | train_loss: 1.1118 | train_acc: 0.2891 | test_loss: 1.1601 | test_acc: 0.1979\nEpoch: 17 | train_loss: 1.1793 | train_acc: 0.2930 | test_loss: 1.1473 | test_acc: 0.2604\nEpoch: 18 | train_loss: 1.1897 | train_acc: 0.3047 | test_loss: 1.0964 | test_acc: 0.2604\nEpoch: 19 | train_loss: 1.1343 | train_acc: 0.2695 | test_loss: 1.0934 | test_acc: 0.1979\nEpoch: 20 | train_loss: 1.1507 | train_acc: 0.2930 | test_loss: 1.2421 | test_acc: 0.1979\nEpoch: 21 | train_loss: 1.0846 | train_acc: 0.4453 | test_loss: 1.1815 | test_acc: 0.2604\nEpoch: 22 | train_loss: 1.1038 | train_acc: 0.4258 | test_loss: 1.1374 | test_acc: 0.2604\nEpoch: 23 | train_loss: 1.1343 | train_acc: 0.3047 | test_loss: 1.1025 | test_acc: 0.2604\nEpoch: 24 | train_loss: 1.1344 | train_acc: 0.2812 | test_loss: 1.0378 | test_acc: 0.5417\nEpoch: 25 | train_loss: 1.1292 | train_acc: 0.2891 | test_loss: 1.1337 | test_acc: 0.2604\nEpoch: 26 | train_loss: 1.0929 | train_acc: 0.4297 | test_loss: 1.0573 | test_acc: 0.5417\nEpoch: 27 | train_loss: 1.1268 | train_acc: 0.2812 | test_loss: 1.0592 | test_acc: 0.5417\nEpoch: 28 | train_loss: 1.1325 | train_acc: 0.3047 | test_loss: 1.1948 | test_acc: 0.1979\nEpoch: 29 | train_loss: 1.1569 | train_acc: 0.2969 | test_loss: 1.1696 | test_acc: 0.2604\nEpoch: 30 | train_loss: 1.0886 | train_acc: 0.4414 | test_loss: 1.0279 | test_acc: 0.5417\nEpoch: 31 | train_loss: 1.1584 | train_acc: 0.2812 | test_loss: 1.0452 | test_acc: 0.5417\nEpoch: 32 | train_loss: 1.0817 | train_acc: 0.4023 | test_loss: 1.2420 | test_acc: 0.1979\nEpoch: 33 | train_loss: 1.1465 | train_acc: 0.2930 | test_loss: 1.2285 | test_acc: 0.1979\nEpoch: 34 | train_loss: 1.1194 | train_acc: 0.3203 | test_loss: 1.1031 | test_acc: 0.2604\nEpoch: 35 | train_loss: 1.0948 | train_acc: 0.3906 | test_loss: 1.0202 | test_acc: 0.5417\nEpoch: 36 | train_loss: 1.1548 | train_acc: 0.2812 | test_loss: 1.0535 | test_acc: 0.5417\nEpoch: 37 | train_loss: 1.1379 | train_acc: 0.3086 | test_loss: 1.1902 | test_acc: 0.1979\nEpoch: 38 | train_loss: 1.1138 | train_acc: 0.2812 | test_loss: 1.0961 | test_acc: 0.1979\nEpoch: 39 | train_loss: 1.0850 | train_acc: 0.4141 | test_loss: 1.1648 | test_acc: 0.1979\nEpoch: 40 | train_loss: 1.0922 | train_acc: 0.4141 | test_loss: 1.1788 | test_acc: 0.1979\nEpoch: 41 | train_loss: 1.1308 | train_acc: 0.2930 | test_loss: 1.1288 | test_acc: 0.1979\nEpoch: 42 | train_loss: 1.1145 | train_acc: 0.2734 | test_loss: 1.0538 | test_acc: 0.5417\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model and save the training results to a dictionary\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/working/S17/super_repo/engine.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, clip_norm)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 181\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(\n\u001b[1;32m    190\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/S17/super_repo/engine.py:71\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device, clip_norm, lr_schedule)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Calculate and accumulate accuracy metric across all batches\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     y_pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43my_pred_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Adjust metrics to get average loss and accuracy per batch\u001b[39;00m\n\u001b[1;32m     74\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}