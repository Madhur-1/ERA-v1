{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:30.981638Z","iopub.execute_input":"2023-09-24T15:37:30.982108Z","iopub.status.idle":"2023-09-24T15:37:31.026648Z","shell.execute_reply.started":"2023-09-24T15:37:30.982064Z","shell.execute_reply":"2023-09-24T15:37:31.025652Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"code","source":"! cd /kaggle/working/\n! cp -r /kaggle/input/erav1-s17/S17 .","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:31.029724Z","iopub.execute_input":"2023-09-24T15:37:31.030041Z","iopub.status.idle":"2023-09-24T15:37:33.616300Z","shell.execute_reply.started":"2023-09-24T15:37:31.030016Z","shell.execute_reply":"2023-09-24T15:37:33.615006Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/S17","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:33.621745Z","iopub.execute_input":"2023-09-24T15:37:33.624206Z","iopub.status.idle":"2023-09-24T15:37:33.697907Z","shell.execute_reply.started":"2023-09-24T15:37:33.624165Z","shell.execute_reply":"2023-09-24T15:37:33.696927Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/working/S17\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom src.model import Transformer\nfrom transformers import AutoTokenizer\nfrom src.gpt_utils import (\n    BATCH_SIZE,\n    BLOCK_SIZE,\n    DEVICE,\n    DROPOUT,\n    LEARNING_RATE,\n    NUM_EMBED,\n    NUM_HEAD,\n    NUM_LAYER,\n    MAX_ITER,\n    EVAL_INTER,\n    encode,\n    decode,\n    get_batch,\n    save_model_to_chekpoint,\n    estimate_loss,\n)\nimport torch.nn as nn\nimport torch.nn.functional as F\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:33.703910Z","iopub.execute_input":"2023-09-24T15:37:33.706366Z","iopub.status.idle":"2023-09-24T15:37:33.765447Z","shell.execute_reply.started":"2023-09-24T15:37:33.706322Z","shell.execute_reply":"2023-09-24T15:37:33.764490Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# raw data\npath_do_data = \"data/english.txt\"\ndata_raw = open(path_do_data, encoding=\"utf-8\").read()\n# we use pretrained BERT tokenizer for performance improvements\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nvocab_size = tokenizer.vocab_size\n# data_raw = data_raw[4000000:] # short dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:33.770233Z","iopub.execute_input":"2023-09-24T15:37:33.772689Z","iopub.status.idle":"2023-09-24T15:37:33.961334Z","shell.execute_reply.started":"2023-09-24T15:37:33.772653Z","shell.execute_reply":"2023-09-24T15:37:33.960344Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# train/val split\ndata = encode(text_seq=data_raw, tokenizer=tokenizer)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:33.965909Z","iopub.execute_input":"2023-09-24T15:37:33.968181Z","iopub.status.idle":"2023-09-24T15:37:34.212618Z","shell.execute_reply.started":"2023-09-24T15:37:33.968146Z","shell.execute_reply":"2023-09-24T15:37:34.211668Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (37443 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Transformer(\n    embed_dim=NUM_EMBED,\n        num_heads=NUM_HEAD,\n        attn_dropout=DROPOUT,\n        mlp_dim=4*NUM_EMBED,\n        mlp_dropout=DROPOUT,\n        mlp_activation=nn.ReLU(),\n        num_layers=NUM_LAYER,\n        embed_dict_size=vocab_size,\n        max_seq_len=BLOCK_SIZE,\n        pad_idx=tokenizer.pad_token_id,\n        add_cls_token=False,\n        pe_requires_grad=False,\n        need_embedding = True,)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:34.213964Z","iopub.execute_input":"2023-09-24T15:37:34.214413Z","iopub.status.idle":"2023-09-24T15:37:35.128810Z","shell.execute_reply.started":"2023-09-24T15:37:34.214379Z","shell.execute_reply":"2023-09-24T15:37:35.127690Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, model, block_size):\n        super(GPT, self).__init__()\n        self.model = model\n        self.linear = nn.Linear(NUM_EMBED, vocab_size, bias=False)\n        # self.linear.weight = self.model.token_embed_layer.weight\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        x = self.model(idx, attn_mask=(self.tril[:T, :T] == 0))\n        x = self.linear(x)\n\n        # compute the loss\n        if targets != None:\n            # cross_entropy accepts inputs in a (batch_size, num_classes)\n            # so we need to reformat our logits dimensions to\n            # (batch_size * time, dim_vocabulary), time = block_size\n            B, T, C = x.shape\n            x = torch.reshape(x, (B * T, C))\n            targets = torch.reshape(targets, (B * T,))\n            loss = F.cross_entropy(x, targets)\n        else:\n            loss = None\n\n        return x, loss","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:35.130288Z","iopub.execute_input":"2023-09-24T15:37:35.131086Z","iopub.status.idle":"2023-09-24T15:37:35.173383Z","shell.execute_reply.started":"2023-09-24T15:37:35.131051Z","shell.execute_reply":"2023-09-24T15:37:35.172257Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# load model to GPU if available\nm = GPT(model, BLOCK_SIZE).to(DEVICE)\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:35.175037Z","iopub.execute_input":"2023-09-24T15:37:35.175393Z","iopub.status.idle":"2023-09-24T15:37:35.507193Z","shell.execute_reply.started":"2023-09-24T15:37:35.175361Z","shell.execute_reply":"2023-09-24T15:37:35.506122Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model with 89.41M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchinfo import summary\nsummary(m)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:35.511089Z","iopub.execute_input":"2023-09-24T15:37:35.511386Z","iopub.status.idle":"2023-09-24T15:37:35.563242Z","shell.execute_reply.started":"2023-09-24T15:37:35.511361Z","shell.execute_reply":"2023-09-24T15:37:35.562298Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"===============================================================================================\nLayer (type:depth-idx)                                                 Param #\n===============================================================================================\nGPT                                                                    --\n├─Transformer: 1-1                                                     --\n│    └─ReLU: 2-1                                                       --\n│    └─Embedding: 2-2                                                  23,440,896\n│    └─PositionalEmbedding: 2-3                                        --\n│    └─Sequential: 2-4                                                 --\n│    │    └─TransformerBlock: 3-1                                      7,087,872\n│    │    └─TransformerBlock: 3-2                                      7,087,872\n│    │    └─TransformerBlock: 3-3                                      7,087,872\n│    │    └─TransformerBlock: 3-4                                      7,087,872\n│    │    └─TransformerBlock: 3-5                                      7,087,872\n│    │    └─TransformerBlock: 3-6                                      7,087,872\n│    └─LayerNorm: 2-5                                                  1,536\n├─Linear: 1-2                                                          23,440,896\n===============================================================================================\nTotal params: 89,410,560\nTrainable params: 89,410,560\nNon-trainable params: 0\n==============================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"# optimizer takes the model's parameters and the learning rate as input,\n# and updates the parameters during the training process in order to\n# minimize the loss function.\noptimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\nMAX_ITER = 500\nfor step in range(MAX_ITER):\n\n    # every EVAL_INTER evaluate the loss on train and val sets\n    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n        loss_train = estimate_loss(\n            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        loss_val = estimate_loss(\n            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n\n    # sample a batch of data\n    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n    logits, loss = m.forward(xb, yb)\n    print(\"step {:10} | loss {:6.4f}\".format(step, loss.item()))\n    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n    optimizer.zero_grad(set_to_none=True)\n    # backward() method on the loss variable calculates the gradients \n    # of the loss with respect to the model's parameters.\n    loss.backward()\n    # step() method on the optimizer updates the model's parameters \n    # using the calculated gradients, in order to minimize the loss.\n    optimizer.step()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:37:35.564790Z","iopub.execute_input":"2023-09-24T15:37:35.565149Z","iopub.status.idle":"2023-09-24T15:38:47.361450Z","shell.execute_reply.started":"2023-09-24T15:37:35.565118Z","shell.execute_reply":"2023-09-24T15:38:47.360341Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"step          0 | train loss 10.5290 | val loss 10.5405\nstep          0 | loss 10.5447\nstep          1 | loss 9.5983\nstep          2 | loss 9.1837\nstep          3 | loss 8.8108\nstep          4 | loss 8.6106\nstep          5 | loss 8.2534\nstep          6 | loss 8.1428\nstep          7 | loss 7.9234\nstep          8 | loss 7.6984\nstep          9 | loss 7.4967\nstep         10 | loss 7.2215\nstep         11 | loss 7.1858\nstep         12 | loss 6.8793\nstep         13 | loss 6.8041\nstep         14 | loss 6.7497\nstep         15 | loss 6.6594\nstep         16 | loss 6.5199\nstep         17 | loss 6.5680\nstep         18 | loss 6.3590\nstep         19 | loss 6.4314\nstep         20 | loss 6.2648\nstep         21 | loss 6.4353\nstep         22 | loss 6.3949\nstep         23 | loss 6.4226\nstep         24 | loss 6.3417\nstep         25 | loss 6.1645\nstep         26 | loss 6.2899\nstep         27 | loss 6.2696\nstep         28 | loss 6.1990\nstep         29 | loss 6.2662\nstep         30 | loss 6.3897\nstep         31 | loss 6.0641\nstep         32 | loss 6.1320\nstep         33 | loss 6.1008\nstep         34 | loss 6.2953\nstep         35 | loss 6.0188\nstep         36 | loss 6.1292\nstep         37 | loss 5.9756\nstep         38 | loss 5.7246\nstep         39 | loss 6.0593\nstep         40 | loss 5.7422\nstep         41 | loss 5.7350\nstep         42 | loss 5.7995\nstep         43 | loss 5.9681\nstep         44 | loss 5.7526\nstep         45 | loss 5.6890\nstep         46 | loss 5.7890\nstep         47 | loss 5.5643\nstep         48 | loss 5.6227\nstep         49 | loss 5.3680\nstep         50 | loss 5.4925\nstep         51 | loss 5.3217\nstep         52 | loss 5.5875\nstep         53 | loss 5.2442\nstep         54 | loss 5.6441\nstep         55 | loss 5.1506\nstep         56 | loss 5.2396\nstep         57 | loss 5.4239\nstep         58 | loss 5.3452\nstep         59 | loss 5.0597\nstep         60 | loss 5.1297\nstep         61 | loss 5.3357\nstep         62 | loss 5.3736\nstep         63 | loss 5.0078\nstep         64 | loss 5.1425\nstep         65 | loss 4.7388\nstep         66 | loss 4.7932\nstep         67 | loss 4.9865\nstep         68 | loss 4.7746\nstep         69 | loss 4.6246\nstep         70 | loss 4.9002\nstep         71 | loss 4.6534\nstep         72 | loss 4.5715\nstep         73 | loss 4.9209\nstep         74 | loss 4.8493\nstep         75 | loss 4.7079\nstep         76 | loss 4.7628\nstep         77 | loss 4.6753\nstep         78 | loss 4.5471\nstep         79 | loss 4.3970\nstep         80 | loss 4.6115\nstep         81 | loss 4.4530\nstep         82 | loss 4.6673\nstep         83 | loss 4.3651\nstep         84 | loss 4.6815\nstep         85 | loss 4.4562\nstep         86 | loss 4.6361\nstep         87 | loss 4.7067\nstep         88 | loss 4.4735\nstep         89 | loss 4.2494\nstep         90 | loss 4.4373\nstep         91 | loss 4.2046\nstep         92 | loss 4.3912\nstep         93 | loss 4.3558\nstep         94 | loss 4.3320\nstep         95 | loss 3.8519\nstep         96 | loss 4.3251\nstep         97 | loss 4.1220\nstep         98 | loss 3.8912\nstep         99 | loss 4.1358\nstep        100 | loss 4.0749\nstep        101 | loss 4.0085\nstep        102 | loss 4.1822\nstep        103 | loss 4.1030\nstep        104 | loss 4.0779\nstep        105 | loss 4.1836\nstep        106 | loss 4.1046\nstep        107 | loss 4.0401\nstep        108 | loss 3.9712\nstep        109 | loss 4.0281\nstep        110 | loss 3.7939\nstep        111 | loss 3.9278\nstep        112 | loss 3.9700\nstep        113 | loss 3.9741\nstep        114 | loss 4.2235\nstep        115 | loss 4.0053\nstep        116 | loss 4.0519\nstep        117 | loss 3.7217\nstep        118 | loss 3.6475\nstep        119 | loss 3.6628\nstep        120 | loss 3.6428\nstep        121 | loss 3.7340\nstep        122 | loss 3.6336\nstep        123 | loss 3.6010\nstep        124 | loss 3.6967\nstep        125 | loss 3.9410\nstep        126 | loss 3.7273\nstep        127 | loss 3.6575\nstep        128 | loss 3.6470\nstep        129 | loss 3.5907\nstep        130 | loss 3.7753\nstep        131 | loss 3.5333\nstep        132 | loss 3.7765\nstep        133 | loss 3.3901\nstep        134 | loss 3.4864\nstep        135 | loss 3.5183\nstep        136 | loss 3.5565\nstep        137 | loss 3.5674\nstep        138 | loss 3.5320\nstep        139 | loss 3.7041\nstep        140 | loss 3.5509\nstep        141 | loss 3.4308\nstep        142 | loss 3.5615\nstep        143 | loss 3.5683\nstep        144 | loss 3.6898\nstep        145 | loss 3.3052\nstep        146 | loss 3.4955\nstep        147 | loss 3.4567\nstep        148 | loss 3.4988\nstep        149 | loss 3.3348\nstep        150 | loss 3.5231\nstep        151 | loss 3.2451\nstep        152 | loss 3.1172\nstep        153 | loss 3.2569\nstep        154 | loss 3.2159\nstep        155 | loss 3.2563\nstep        156 | loss 3.0981\nstep        157 | loss 3.1960\nstep        158 | loss 3.3499\nstep        159 | loss 3.1752\nstep        160 | loss 3.2807\nstep        161 | loss 2.9755\nstep        162 | loss 3.0369\nstep        163 | loss 2.7934\nstep        164 | loss 3.3276\nstep        165 | loss 3.2098\nstep        166 | loss 2.9579\nstep        167 | loss 3.0978\nstep        168 | loss 3.2758\nstep        169 | loss 3.1279\nstep        170 | loss 2.9780\nstep        171 | loss 3.0012\nstep        172 | loss 3.1398\nstep        173 | loss 3.0162\nstep        174 | loss 2.8819\nstep        175 | loss 2.8889\nstep        176 | loss 2.9887\nstep        177 | loss 2.6014\nstep        178 | loss 2.5889\nstep        179 | loss 2.6364\nstep        180 | loss 2.8028\nstep        181 | loss 2.8745\nstep        182 | loss 2.7662\nstep        183 | loss 2.8991\nstep        184 | loss 2.8616\nstep        185 | loss 2.6150\nstep        186 | loss 2.7614\nstep        187 | loss 2.5405\nstep        188 | loss 2.7044\nstep        189 | loss 2.5958\nstep        190 | loss 2.8121\nstep        191 | loss 2.6319\nstep        192 | loss 2.5638\nstep        193 | loss 2.8883\nstep        194 | loss 2.6795\nstep        195 | loss 2.4190\nstep        196 | loss 2.5768\nstep        197 | loss 2.4256\nstep        198 | loss 2.3998\nstep        199 | loss 2.3582\nstep        200 | loss 2.6010\nstep        201 | loss 2.7278\nstep        202 | loss 2.6817\nstep        203 | loss 2.4361\nstep        204 | loss 2.5414\nstep        205 | loss 2.5014\nstep        206 | loss 2.4201\nstep        207 | loss 2.2394\nstep        208 | loss 2.6407\nstep        209 | loss 2.4418\nstep        210 | loss 2.5000\nstep        211 | loss 2.4228\nstep        212 | loss 2.3296\nstep        213 | loss 2.3706\nstep        214 | loss 2.2674\nstep        215 | loss 2.2497\nstep        216 | loss 2.1649\nstep        217 | loss 2.1421\nstep        218 | loss 2.3491\nstep        219 | loss 2.1338\nstep        220 | loss 2.0845\nstep        221 | loss 2.0433\nstep        222 | loss 2.2313\nstep        223 | loss 2.1336\nstep        224 | loss 2.0878\nstep        225 | loss 2.1371\nstep        226 | loss 2.2402\nstep        227 | loss 1.9173\nstep        228 | loss 2.0934\nstep        229 | loss 1.9526\nstep        230 | loss 2.0687\nstep        231 | loss 2.1874\nstep        232 | loss 1.9280\nstep        233 | loss 2.0157\nstep        234 | loss 1.8816\nstep        235 | loss 2.0342\nstep        236 | loss 1.7867\nstep        237 | loss 2.0351\nstep        238 | loss 1.7991\nstep        239 | loss 1.8567\nstep        240 | loss 2.0778\nstep        241 | loss 1.8993\nstep        242 | loss 1.9299\nstep        243 | loss 1.6622\nstep        244 | loss 1.7751\nstep        245 | loss 1.9077\nstep        246 | loss 1.8120\nstep        247 | loss 1.6755\nstep        248 | loss 1.7921\nstep        249 | loss 1.5999\nstep        250 | loss 1.8200\nstep        251 | loss 1.8021\nstep        252 | loss 1.7698\nstep        253 | loss 1.7376\nstep        254 | loss 1.5867\nstep        255 | loss 1.7323\nstep        256 | loss 1.5900\nstep        257 | loss 1.5996\nstep        258 | loss 1.6843\nstep        259 | loss 1.7602\nstep        260 | loss 1.5943\nstep        261 | loss 1.5180\nstep        262 | loss 1.5605\nstep        263 | loss 1.4162\nstep        264 | loss 1.5203\nstep        265 | loss 1.4881\nstep        266 | loss 1.6495\nstep        267 | loss 1.3692\nstep        268 | loss 1.4894\nstep        269 | loss 1.5213\nstep        270 | loss 1.4356\nstep        271 | loss 1.5174\nstep        272 | loss 1.4913\nstep        273 | loss 1.3400\nstep        274 | loss 1.3691\nstep        275 | loss 1.4255\nstep        276 | loss 1.5104\nstep        277 | loss 1.2442\nstep        278 | loss 1.4793\nstep        279 | loss 1.3851\nstep        280 | loss 1.4725\nstep        281 | loss 1.2501\nstep        282 | loss 1.4372\nstep        283 | loss 1.2799\nstep        284 | loss 1.2914\nstep        285 | loss 1.3217\nstep        286 | loss 1.2051\nstep        287 | loss 1.2932\nstep        288 | loss 1.2674\nstep        289 | loss 1.1209\nstep        290 | loss 1.3561\nstep        291 | loss 1.2773\nstep        292 | loss 1.2538\nstep        293 | loss 1.2280\nstep        294 | loss 1.1539\nstep        295 | loss 1.3180\nstep        296 | loss 1.1639\nstep        297 | loss 1.2199\nstep        298 | loss 1.1945\nstep        299 | loss 1.1460\nstep        300 | loss 1.1322\nstep        301 | loss 1.0196\nstep        302 | loss 1.1205\nstep        303 | loss 1.0978\nstep        304 | loss 1.0644\nstep        305 | loss 0.9283\nstep        306 | loss 1.0146\nstep        307 | loss 1.1033\nstep        308 | loss 0.9946\nstep        309 | loss 1.0444\nstep        310 | loss 1.0513\nstep        311 | loss 0.9639\nstep        312 | loss 1.0049\nstep        313 | loss 0.9254\nstep        314 | loss 1.0619\nstep        315 | loss 1.0909\nstep        316 | loss 1.0360\nstep        317 | loss 0.8800\nstep        318 | loss 0.9210\nstep        319 | loss 1.0486\nstep        320 | loss 1.0387\nstep        321 | loss 0.9085\nstep        322 | loss 1.0275\nstep        323 | loss 1.0066\nstep        324 | loss 0.8819\nstep        325 | loss 1.0417\nstep        326 | loss 0.9229\nstep        327 | loss 0.8142\nstep        328 | loss 0.9890\nstep        329 | loss 0.8040\nstep        330 | loss 0.8420\nstep        331 | loss 0.8160\nstep        332 | loss 0.8557\nstep        333 | loss 0.8903\nstep        334 | loss 0.7356\nstep        335 | loss 0.8415\nstep        336 | loss 0.8282\nstep        337 | loss 0.6341\nstep        338 | loss 0.9442\nstep        339 | loss 0.7760\nstep        340 | loss 0.7513\nstep        341 | loss 0.7704\nstep        342 | loss 0.8256\nstep        343 | loss 0.8598\nstep        344 | loss 0.7739\nstep        345 | loss 0.8003\nstep        346 | loss 0.7752\nstep        347 | loss 0.7322\nstep        348 | loss 0.7739\nstep        349 | loss 0.6980\nstep        350 | loss 0.7112\nstep        351 | loss 0.7676\nstep        352 | loss 0.6323\nstep        353 | loss 0.7207\nstep        354 | loss 0.7011\nstep        355 | loss 0.6455\nstep        356 | loss 0.7991\nstep        357 | loss 0.7213\nstep        358 | loss 0.6847\nstep        359 | loss 0.6738\nstep        360 | loss 0.7264\nstep        361 | loss 0.6523\nstep        362 | loss 0.7160\nstep        363 | loss 0.7073\nstep        364 | loss 0.6465\nstep        365 | loss 0.6208\nstep        366 | loss 0.6457\nstep        367 | loss 0.5592\nstep        368 | loss 0.7526\nstep        369 | loss 0.6859\nstep        370 | loss 0.6153\nstep        371 | loss 0.6912\nstep        372 | loss 0.6937\nstep        373 | loss 0.6844\nstep        374 | loss 0.5605\nstep        375 | loss 0.5528\nstep        376 | loss 0.6116\nstep        377 | loss 0.5711\nstep        378 | loss 0.6338\nstep        379 | loss 0.5798\nstep        380 | loss 0.6230\nstep        381 | loss 0.5564\nstep        382 | loss 0.5712\nstep        383 | loss 0.6379\nstep        384 | loss 0.5614\nstep        385 | loss 0.5895\nstep        386 | loss 0.4856\nstep        387 | loss 0.5176\nstep        388 | loss 0.4603\nstep        389 | loss 0.6480\nstep        390 | loss 0.5741\nstep        391 | loss 0.5238\nstep        392 | loss 0.5108\nstep        393 | loss 0.5323\nstep        394 | loss 0.5870\nstep        395 | loss 0.4907\nstep        396 | loss 0.5279\nstep        397 | loss 0.5597\nstep        398 | loss 0.5131\nstep        399 | loss 0.4791\nstep        400 | loss 0.5148\nstep        401 | loss 0.5744\nstep        402 | loss 0.5668\nstep        403 | loss 0.5224\nstep        404 | loss 0.4652\nstep        405 | loss 0.4447\nstep        406 | loss 0.4686\nstep        407 | loss 0.4885\nstep        408 | loss 0.5444\nstep        409 | loss 0.4380\nstep        410 | loss 0.5071\nstep        411 | loss 0.4489\nstep        412 | loss 0.4985\nstep        413 | loss 0.4810\nstep        414 | loss 0.5410\nstep        415 | loss 0.4693\nstep        416 | loss 0.4532\nstep        417 | loss 0.4703\nstep        418 | loss 0.4330\nstep        419 | loss 0.4134\nstep        420 | loss 0.4598\nstep        421 | loss 0.4652\nstep        422 | loss 0.4835\nstep        423 | loss 0.4332\nstep        424 | loss 0.4238\nstep        425 | loss 0.4313\nstep        426 | loss 0.4541\nstep        427 | loss 0.4654\nstep        428 | loss 0.4881\nstep        429 | loss 0.4452\nstep        430 | loss 0.4548\nstep        431 | loss 0.4165\nstep        432 | loss 0.4027\nstep        433 | loss 0.4202\nstep        434 | loss 0.3811\nstep        435 | loss 0.3763\nstep        436 | loss 0.4419\nstep        437 | loss 0.4716\nstep        438 | loss 0.4290\nstep        439 | loss 0.4053\nstep        440 | loss 0.4019\nstep        441 | loss 0.3829\nstep        442 | loss 0.4836\nstep        443 | loss 0.3957\nstep        444 | loss 0.4427\nstep        445 | loss 0.4602\nstep        446 | loss 0.4059\nstep        447 | loss 0.4285\nstep        448 | loss 0.3779\nstep        449 | loss 0.4175\nstep        450 | loss 0.4418\nstep        451 | loss 0.3834\nstep        452 | loss 0.4341\nstep        453 | loss 0.4537\nstep        454 | loss 0.3371\nstep        455 | loss 0.3931\nstep        456 | loss 0.3747\nstep        457 | loss 0.3936\nstep        458 | loss 0.3463\nstep        459 | loss 0.4057\nstep        460 | loss 0.3425\nstep        461 | loss 0.3832\nstep        462 | loss 0.3830\nstep        463 | loss 0.4011\nstep        464 | loss 0.3936\nstep        465 | loss 0.3821\nstep        466 | loss 0.4207\nstep        467 | loss 0.3564\nstep        468 | loss 0.3219\nstep        469 | loss 0.3447\nstep        470 | loss 0.3577\nstep        471 | loss 0.3370\nstep        472 | loss 0.4545\nstep        473 | loss 0.3397\nstep        474 | loss 0.3321\nstep        475 | loss 0.3419\nstep        476 | loss 0.3399\nstep        477 | loss 0.3936\nstep        478 | loss 0.3481\nstep        479 | loss 0.3673\nstep        480 | loss 0.3483\nstep        481 | loss 0.3479\nstep        482 | loss 0.3520\nstep        483 | loss 0.3587\nstep        484 | loss 0.3433\nstep        485 | loss 0.3411\nstep        486 | loss 0.3224\nstep        487 | loss 0.3393\nstep        488 | loss 0.3368\nstep        489 | loss 0.3672\nstep        490 | loss 0.3318\nstep        491 | loss 0.3594\nstep        492 | loss 0.3754\nstep        493 | loss 0.3052\nstep        494 | loss 0.3041\nstep        495 | loss 0.3151\nstep        496 | loss 0.3541\nstep        497 | loss 0.3129\nstep        498 | loss 0.3089\nstep        499 | train loss 0.2291 | val loss 7.0340\nstep        499 | loss 0.3225\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}