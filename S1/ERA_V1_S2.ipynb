{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhur-1/ERA-v1/blob/master/ERA_V1_S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "2d6a880e-a90b-406d-8c9b-d37c83afa774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for a CUDA device (GPU)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# Selects the device based on GPU availability between CPU & GPU\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Displays the available device\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "8a654993-f00f-4d92-bb82-97df1ead0f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images to process at once in a mini-batch\n",
        "batch_size = 128\n",
        "\n",
        "# Gives us the DataLoader object which is an iterator over the MNIST dataset and\n",
        "# gives batches of batch_size. It is also responsible for shuffling the dataset\n",
        "# The transforms used are conversion of array to tensor and normalization to fixed\n",
        "# distribution (given by the mean and std. dev.)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Gives us the DataLoader object for the test dataset in the same manner\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08786c24-b909-41d5-ba8d-db9ee391f517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 319909136.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 126843658.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 113310254.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 23902796.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far. \n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\". "
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FirstDNN class inherited from the nn.Module class which is used for defining\n",
        "the pyTorch CNN model.\n",
        "\"\"\"\n",
        "class FirstDNN(nn.Module):\n",
        "  # The initializer for the class contains the model layers with different parameters\n",
        "  # This is reponsible for initializing the model parameters when the model object\n",
        "  # is instantiated\n",
        "  def __init__(self):\n",
        "    # initializes the super class nn.Module from which it is inherited\n",
        "    super(FirstDNN, self).__init__()\n",
        "\n",
        "    # This is a 2d Convolution layer with parameters as\n",
        "    #(in_channels = 1, out_channels (No. of filters) = 32, kernel_size = 3x3, padding = 1)\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "    # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    \n",
        "    # 2d MaxPooling layer with parameters as (kernel_size=2, stride=2)\n",
        "    # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "    \n",
        "    # These layers do not have a padding now (padding=0 default!)\n",
        "    # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out:16 , n_out:7 , j_out:4\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:16 , n_in:7 , j_in:4, s:1 , r_out:24 , n_out:5 , j_out:4\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "    # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "    # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out:1 , j_out:4\n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "  # ForwardPass method of the model\n",
        "  # Used to obtain output of a model given the current state and input\n",
        "  def forward(self, x):\n",
        "    # The input x is fed into conv1 layer followed by a relu non-linearity\n",
        "    # Followed by conv2 -> relu -> pool1 -> conv3 -> relu -> conv4 -> relu -> pool2\n",
        "    # Followed by conv7 -> relu -> flatten -> log_softmax (A non-linearity with normalization benefits)\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "    x = F.relu(self.conv7(x))\n",
        "    x = x.view(-1, 10)\n",
        "    return F.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the model to the current device\n",
        "model = FirstDNN().to(device)"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the summary of the model\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "173e98e3-d2f0-4cdd-eaac-b03397644f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-ce156e6a5dcd>:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\"\"\"\n",
        "Train method responsible for training (weight optimization) of the input\n",
        "model object\n",
        "\n",
        "Inputs:\n",
        "model: model object\n",
        "device: current device\n",
        "train_loader: train dataloader object\n",
        "optimizer: optimization method used for NN training\n",
        "epoch: Current epoch number\n",
        "\"\"\"\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to train (changes behaviour of various layers like batch-norm)\n",
        "    model.train()\n",
        "    # Progress bar using tqdm python module\n",
        "    pbar = tqdm(train_loader)\n",
        "    # Get the (data, target) batch tuple from train_loader with index as batch_idx\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # Send the data and target to the current device, necessary for communication with the model\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Clear previous gradients before new gradient computation as optimizer tends to \n",
        "        # accumulate gradients by default\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform the forward pass on the dataset, getting the model outputs for the inputs\n",
        "        output = model(data)\n",
        "\n",
        "        # Compare the computed outputs with the target using the NLL (Negative Log Likelihood, multi-class classification loss) fxn\n",
        "        # This returns the loss per input in the batch\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        \n",
        "        # Computed the model weight update gradients (dL/dW) using the loss computed in the previous step\n",
        "        loss.backward()\n",
        "        \n",
        "        # Performs the weight update (Wnew = Wold - alpha*(dL/dWold))\n",
        "        optimizer.step()\n",
        "\n",
        "        # Prints the progress of the training given by the loss values per batch\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Test method responsible for inference on the learnt model parameters\n",
        "\n",
        "Inputs:\n",
        "model: model object\n",
        "device: current device\n",
        "test_loader: test dataloader object (data to be tested upon)\n",
        "\"\"\"\n",
        "def test(model, device, test_loader):\n",
        "    # Set the model to eval mode (changes behaviour of various layers like batch-norm)\n",
        "    model.eval()\n",
        "    # Initialize the global test loss to zero\n",
        "    test_loss = 0\n",
        "    \n",
        "    # Initialize global correct examples to zero (Used for accuracy calc)\n",
        "    correct = 0\n",
        "\n",
        "    # We explicitly tell pytorch to not compute the gradient to save compute!\n",
        "    with torch.no_grad():\n",
        "        # Get the (data, target)batch tuple from the loader\n",
        "        for data, target in test_loader:\n",
        "            # Send the data and target to the current device, necessary for communication with the model\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Perform the forward pass on the dataset, getting the model outputs for the inputs\n",
        "            output = model(data)\n",
        "\n",
        "            # Compare the computed outputs with the target using the NLLfxn\n",
        "            # The reduction method specifies summing the loss values for all the examples in the batch\n",
        "            # Finally the item() method specifies to just get the loss value from the tensor object\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            \n",
        "            # Get the predicted output from the maximum probable output\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            \n",
        "            # Compare the predictions with the target to get the number of correct examples\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Normalize the test loss with the length of the dataset\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Print the test stats - Loss and Accuracy (No. of correct normalized by total number)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the optimization method for the model parameters. The learning rate\n",
        "# is specified to 0.01 with a momentum of 0.9\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Run the training and testing for one epoch\n",
        "for epoch in range(1, 5):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "69e5450c-b6a2-4fb5-db63-fefac40c490c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-15-ce156e6a5dcd>:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=1.054046630859375 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9651, Accuracy: 6909/10000 (69%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.8109932541847229 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7343, Accuracy: 7876/10000 (79%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.7731958031654358 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7197, Accuracy: 7926/10000 (79%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.794904887676239 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7114, Accuracy: 7929/10000 (79%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reIBU667OG_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}